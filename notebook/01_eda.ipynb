{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb6a0b0-b622-448f-8c1b-130f2a6e1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add project root to Python path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils import *\n",
    "from src.features import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a412ca-3322-4402-add7-8027583fa84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_train_data()\n",
    "train = train_df.copy()\n",
    "test_df = load_test_data()\n",
    "test = test_df.copy()\n",
    "train = train.drop(columns='id')\n",
    "test = test.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259659ed-7bd8-46e6-894e-d5c2e16fda76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0       0          2              2          5              1              0   \n",
       "1       0          1              1          7              0              0   \n",
       "2       0          5              4          9              1              0   \n",
       "3       0          0              1          2              0              0   \n",
       "4       0          0              2          0              1              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "0              0              1              0              0              0   \n",
       "1              0              0              1              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              1              0              0              0              0   \n",
       "4              1              0              0              0              0   \n",
       "\n",
       "   ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  ps_ind_15  \\\n",
       "0              0              0              0          0         11   \n",
       "1              0              0              0          0          3   \n",
       "2              0              0              0          0         12   \n",
       "3              0              0              0          0          8   \n",
       "4              0              0              0          0          9   \n",
       "\n",
       "   ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  ps_reg_02  \\\n",
       "0              0              1              0        0.7        0.2   \n",
       "1              0              0              1        0.8        0.4   \n",
       "2              1              0              0        0.0        0.0   \n",
       "3              1              0              0        0.9        0.2   \n",
       "4              1              0              0        0.7        0.6   \n",
       "\n",
       "   ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  \\\n",
       "0   0.718070             10              1             -1              0   \n",
       "1   0.766078             11              1             -1              0   \n",
       "2  -1.000000              7              1             -1              0   \n",
       "3   0.580948              7              1              0              0   \n",
       "4   0.840759             11              1             -1              0   \n",
       "\n",
       "   ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  \\\n",
       "0              1              4              1              0              0   \n",
       "1             -1             11              1              1              2   \n",
       "2             -1             14              1              1              2   \n",
       "3              1             11              1              1              3   \n",
       "4             -1             14              1              1              2   \n",
       "\n",
       "   ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  ps_car_13  ps_car_14  \\\n",
       "0              1             12          2   0.400000   0.883679   0.370810   \n",
       "1              1             19          3   0.316228   0.618817   0.388716   \n",
       "2              1             60          1   0.316228   0.641586   0.347275   \n",
       "3              1            104          1   0.374166   0.542949   0.294958   \n",
       "4              1             82          3   0.316070   0.565832   0.365103   \n",
       "\n",
       "   ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  ps_calc_04  ps_calc_05  \\\n",
       "0   3.605551         0.6         0.5         0.2           3           1   \n",
       "1   2.449490         0.3         0.1         0.3           2           1   \n",
       "2   3.316625         0.5         0.7         0.1           2           2   \n",
       "3   2.000000         0.6         0.9         0.1           2           4   \n",
       "4   2.000000         0.4         0.6         0.0           2           2   \n",
       "\n",
       "   ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  ps_calc_10  ps_calc_11  \\\n",
       "0          10           1          10           1           5           9   \n",
       "1           9           5           8           1           7           3   \n",
       "2           9           1           8           2           7           4   \n",
       "3           7           1           8           4           2           2   \n",
       "4           6           3          10           2          12           3   \n",
       "\n",
       "   ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "0           1           5           8               0               1   \n",
       "1           1           1           9               0               1   \n",
       "2           2           7           7               0               1   \n",
       "3           2           4           9               0               0   \n",
       "4           1           1           3               0               0   \n",
       "\n",
       "   ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "0               1               0               0               1  \n",
       "1               1               0               1               0  \n",
       "2               1               0               1               0  \n",
       "3               0               0               0               0  \n",
       "4               0               1               1               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59251049-4928-4f58-8673-8ad7733ed0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumetable2(df, target_col, missing_value=-1, ignore_cols=None, verbose=True):\n",
    "    ignore_cols = ignore_cols or []\n",
    "    if verbose:\n",
    "        print(f'Data shape: {df.shape}')\n",
    "\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['Data Type'])\n",
    "    summary['Missing'] = (df == missing_value).sum().values\n",
    "    summary['Nunique'] = df.nunique().values\n",
    "    summary['Feature Type'] = None\n",
    "\n",
    "    for col in df.columns:\n",
    "        if 'target' in col:\n",
    "            summary.loc[col, 'Feature Type'] = 'Target'\n",
    "        elif 'bin' in col:\n",
    "            summary.loc[col, 'Feature Type'] = 'Binary'\n",
    "        elif 'cat' in col:\n",
    "            summary.loc[col, 'Feature Type'] = 'Categorical'\n",
    "        else:\n",
    "            summary.loc[col, 'Feature Type'] = 'Temp'\n",
    "            \n",
    "    summary = summary.sort_values(by='Feature Type')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ce718e-d92d-4eda-bf61-29dab92c44da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (595212, 58)\n"
     ]
    }
   ],
   "source": [
    "feature_table = resumetable2(train, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b17e9e3-8127-4633-807e-b9e37b20e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = feature_table[feature_table['Feature Type'] == 'Categorical'].index.tolist()\n",
    "cat_cols = cat_cols + feature_table[feature_table['Feature Type'] == 'Binary'].index.tolist()\n",
    "cat_cols = cat_cols + feature_table[(feature_table['Nunique'] < 30) & (feature_table['Feature Type'] == 'Temp')].index.tolist()\n",
    "num_cols = feature_table[(feature_table['Nunique'] >= 30) & (feature_table['Feature Type'] == 'Temp')].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d663082c-e2bd-4bef-8ca8-4fd9a15e4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    573518\n",
       "1     21694\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f936fa9-d102-48d4-893a-24532f142ce8",
   "metadata": {},
   "source": [
    "1. value_counts: stratified kfold 사용 필요, 타겟값 불균형\n",
    "2. target 데이터부터 시각화 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5decc63d-dd02-4d74-b09f-b3671ec5fcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=train, x='target')\n",
    "plt.title('Target Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a00570f-e271-4079-8c30-7a8732edf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def gini_normalized(y_true, y_pred):\n",
    "    return 2 * roc_auc_score(y_true, y_pred) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5220def4-f81b-4efe-9c0f-d30c1f20fc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1365\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Fold 1 Gini Normalized:  0.28310\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1367\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Fold 2 Gini Normalized:  0.27432\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1366\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Fold 3 Gini Normalized:  0.27938\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014288 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1365\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Fold 4 Gini Normalized:  0.27225\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015275 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1367\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Fold 5 Gini Normalized:  0.26293\n",
      "Baseline CV Score:  0.27440 +/-  0.00688\n"
     ]
    }
   ],
   "source": [
    "# Pipeline -> Baseline model\n",
    "\n",
    "# 타겟 정의  \n",
    "X = train.drop(columns='target')\n",
    "y = train['target']\n",
    "\n",
    "# 전처리기 & 피처 생성기 정의 \n",
    "interaction_fe = InteractionFeatureGenerator()\n",
    "\n",
    "# 모델 정의 \n",
    "model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# 파이프라인 생성 \n",
    "pipeline = make_pipeline(\n",
    "    interaction_fe,\n",
    "    model\n",
    ")\n",
    "\n",
    "# 교차 검증 및 평가 \n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "    score = gini_normalized(y_val.values, y_pred_proba)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f'Fold {fold+1} Gini Normalized: {score: .5f}')\n",
    "    \n",
    "# 결과\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "std_cv_score = np.std(cv_scores)\n",
    "\n",
    "print(f'Baseline CV Score: {mean_cv_score: .5f} +/- {std_cv_score: .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ab18b-bd4c-4e7b-a3a7-00ea952eca44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### model.predict_proba()\n",
    "이진분류에서 predict_proba()는 2차원 배열을 반환 <br>\n",
    "y_pred_proba = model.predict_proba(X_val)<br>\n",
    "print(y_pred_proba.shape)  # (n_samples, 2)<br>\n",
    "print(y_pred_proba[:3])    # 처음 3개 샘플 예시<br>\n",
    "\n",
    "출력 예시:\n",
    "\n",
    "[[0.8, 0.2],   첫 번째 샘플: 80% 확률로 class 0, 20% 확률로 class 1 <br>\n",
    " [0.3, 0.7],   두 번째 샘플: 30% 확률로 class 0, 70% 확률로 class 1 <br>\n",
    " [0.9, 0.1]]   세 번째 샘플: 90% 확률로 class 0, 10% 확률로 class 1 <br>\n",
    " \n",
    "\n",
    "[:, 1]의 의미:<br>\n",
    "첫 번째 차원(행): 모든 샘플 선택 (:)<br>\n",
    "두 번째 차원(열): index 1 선택 (1) = class 1의 확률<br>\n",
    "y_pred_proba[:, 1]  # class 1 확률 (사고 날 확률) <- 우리가 원하는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b932c18-998c-419a-a58e-f30fe55f0d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param_sets = [\n",
    "#     # Baseline (현재)\n",
    "#     {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': -1},\n",
    "    \n",
    "#     # Set 1: 더 많은 트리, 낮은 학습률\n",
    "#     {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth': 7, 'num_leaves': 64},\n",
    "    \n",
    "#     # Set 2: 더 깊은 트리\n",
    "#     {'n_estimators': 1500, 'learning_rate': 0.03, 'max_depth': 8, 'num_leaves': 100},\n",
    "    \n",
    "#     # Set 3: 정규화 강화\n",
    "#     {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth': 6, 'num_leaves': 50, \n",
    "#      'min_child_samples': 100, 'subsample': 0.8, 'colsample_bytree': 0.8}\n",
    "# ]\n",
    "\n",
    "# best_score = 0\n",
    "# best_params = None\n",
    "\n",
    "# for i, params in enumerate(param_sets):\n",
    "#     print(f\"\\n=== Testing Parameter Set {i+1} ===\")\n",
    "#     print(f\"Params: {params}\")\n",
    "    \n",
    "#     # 모델 생성\n",
    "#     model = LGBMClassifier(random_state=42, **params)\n",
    "    \n",
    "#     # 파이프라인 생성\n",
    "#     pipeline = make_pipeline(interaction_fe, model)\n",
    "    \n",
    "#     # CV 실행\n",
    "#     cv_scores = []\n",
    "#     for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "#         X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "#         X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "#         pipeline.fit(X_train, y_train)\n",
    "#         y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "#         score = gini_normalized(y_val.values, y_pred_proba)\n",
    "#         cv_scores.append(score)\n",
    "    \n",
    "#     # 결과 출력\n",
    "#     mean_score = np.mean(cv_scores)\n",
    "#     std_score = np.std(cv_scores)\n",
    "#     print(f\"CV Score: {mean_score:.5f} +/- {std_score:.5f}\")\n",
    "    \n",
    "#     # 최고 점수 업데이트\n",
    "#     if mean_score > best_score:\n",
    "#         best_score = mean_score\n",
    "#         best_params = params\n",
    "        \n",
    "# print(f\"\\n=== BEST RESULT ===\")\n",
    "# print(f\"Best Score: {best_score:.5f}\")\n",
    "# print(f\"Best Params: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1b4572-f761-43d6-a7bd-4d2c8dfe7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_normalized(y_true, y_pred):\n",
    "    return 2 * roc_auc_score(y_true, y_pred) - 1\n",
    "\n",
    "def quick_cv_test(train, feature_name, baseline_score=0.27440):\n",
    "    train_test = train.copy()\n",
    "    \n",
    "    # 타겟 분리\n",
    "    X = train_test.drop(columns='target')\n",
    "    y = train_test['target']\n",
    "    \n",
    "    # 모델 및 CV 설정\n",
    "    model = LGBMClassifier(random_state=42)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CV 실행\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        score = gini_normalized(y_val.values, y_pred)\n",
    "        cv_scores.append(score)\n",
    "    \n",
    "    mean_score = np.mean(cv_scores)\n",
    "    \n",
    "    # 결과 출력\n",
    "    improvement = mean_score - baseline_score\n",
    "    print(f'{feature_name}: {mean_score:.5f} ({improvement:+.5f})')\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698dcd1b-92b4-4a41-a0e6-2447bd6ed79a",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e13430-b38c-4845-896f-877bddb4a2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target           1.000000\n",
      "ps_car_13        0.053899\n",
      "ps_car_12        0.038790\n",
      "ps_ind_17_bin    0.037053\n",
      "ps_car_07_cat    0.036395\n",
      "ps_reg_02        0.034800\n",
      "ps_ind_07_bin    0.034218\n",
      "ps_ind_06_bin    0.034017\n",
      "ps_car_04_cat    0.032900\n",
      "ps_car_03_cat    0.032401\n",
      "ps_car_02_cat    0.031534\n",
      "ps_reg_03        0.030888\n",
      "ps_ind_05_cat    0.029165\n",
      "ps_ind_16_bin    0.027778\n",
      "ps_car_15        0.027667\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_with_target = train.corr()['target'].abs().sort_values(ascending=False)\n",
    "# .abs(): 절댓값 제시, 타겟 예측에는 절댓값이 중요 \n",
    "print(corr_with_target.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17fbb1d6-4552-43ee-b7fd-d17a978c094b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Fold 1 Gini Normalized:  0.28617\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Fold 2 Gini Normalized:  0.27630\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1621\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Fold 3 Gini Normalized:  0.28175\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Fold 4 Gini Normalized:  0.27353\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Fold 5 Gini Normalized:  0.26104\n",
      "Feature Engineered CV Score:  0.27576 +/-  0.00856\n"
     ]
    }
   ],
   "source": [
    "# 원본으로 초기화\n",
    "train = train_df.drop(columns=['id']).copy()\n",
    "\n",
    "## Feature Engineering으로 무한 머리박기 \n",
    "train['reg03/reg02'] = train['ps_reg_03'] / (train['ps_reg_02'] + 1e-8)\n",
    "\n",
    "\n",
    "# 타겟 정의  \n",
    "X = train.drop(columns='target')\n",
    "y = train['target']\n",
    "\n",
    "# 전처리기 & 피처 생성기 정의 \n",
    "# interaction_fe = InteractionFeatureGenerator()\n",
    "\n",
    "# 모델 정의 / 머리 박아보면서 feature engineering 검증 -> 파이프라인 없이 과정 최소화하며 바로 검증 \n",
    "model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# 교차 검증 및 평가 \n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    score = gini_normalized(y_val.values, y_pred_proba)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f'Fold {fold+1} Gini Normalized: {score: .5f}')\n",
    "    \n",
    "# 결과\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "std_cv_score = np.std(cv_scores)\n",
    "\n",
    "print(f'Feature Engineered CV Score: {mean_cv_score: .5f} +/- {std_cv_score: .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bd058-6bcf-4fa2-bc58-f2d15d625b1b",
   "metadata": {},
   "source": [
    "### 분석\n",
    "1. ps_ind_xx_bin: 거의 대부분의 feature가 의미 적음 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49cbc5ea-b34a-4cad-95f4-a04c5ad55e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036852 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "gain_importance = model.booster_.feature_importance(importance_type='gain')\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': gain_importance\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe34c5f0-e6cf-4856-b930-0b92891bc186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ps_ind_13_bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ps_ind_10_bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ps_ind_11_bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature  importance\n",
       "12  ps_ind_13_bin         0.0\n",
       "9   ps_ind_10_bin         0.0\n",
       "10  ps_ind_11_bin         0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = feature_importance_df.head(100)\n",
    "low_features = feature_importance_df.tail(3)\n",
    "low_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ed064a-9b0f-41fd-b715-0228cc4ec08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>reg03/reg02</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_calc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ps_car_13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048308</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>-0.015259</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>-0.037310</td>\n",
       "      <td>0.194316</td>\n",
       "      <td>0.099752</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>-0.223412</td>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.184690</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_03</th>\n",
       "      <td>0.048308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>0.170449</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>0.056337</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_03</th>\n",
       "      <td>0.099415</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>0.271879</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>0.139125</td>\n",
       "      <td>0.230339</td>\n",
       "      <td>-0.053356</td>\n",
       "      <td>0.637035</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>-0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>-0.036213</td>\n",
       "      <td>0.053777</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>-0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_15</th>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.170449</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.059703</td>\n",
       "      <td>-0.051826</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>-0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>0.101257</td>\n",
       "      <td>0.058876</td>\n",
       "      <td>-0.043662</td>\n",
       "      <td>0.069967</td>\n",
       "      <td>0.137328</td>\n",
       "      <td>-0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg03/reg02</th>\n",
       "      <td>0.008900</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.271879</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>-0.161803</td>\n",
       "      <td>-0.003786</td>\n",
       "      <td>0.003034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_14</th>\n",
       "      <td>-0.037310</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>-0.163385</td>\n",
       "      <td>-0.002534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_02</th>\n",
       "      <td>0.194316</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183855</td>\n",
       "      <td>0.231710</td>\n",
       "      <td>-0.069651</td>\n",
       "      <td>0.471027</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_01</th>\n",
       "      <td>0.099752</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>0.139125</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>0.101257</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.183855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056121</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>-0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <td>0.167127</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>0.230339</td>\n",
       "      <td>-0.036213</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.058876</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>0.231710</td>\n",
       "      <td>-0.056121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.184266</td>\n",
       "      <td>0.138055</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <td>-0.223412</td>\n",
       "      <td>0.056337</td>\n",
       "      <td>-0.053356</td>\n",
       "      <td>0.053777</td>\n",
       "      <td>0.059703</td>\n",
       "      <td>-0.043662</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>-0.069651</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045541</td>\n",
       "      <td>-0.148339</td>\n",
       "      <td>-0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_01</th>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.637035</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.051826</td>\n",
       "      <td>0.069967</td>\n",
       "      <td>-0.161803</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.471027</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.184266</td>\n",
       "      <td>-0.045541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>-0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <td>0.184690</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>0.137328</td>\n",
       "      <td>-0.003786</td>\n",
       "      <td>-0.163385</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.138055</td>\n",
       "      <td>-0.148339</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_10</th>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ps_car_13  ps_ind_03  ps_reg_03  ps_ind_05_cat  ps_ind_15  \\\n",
       "ps_car_13       1.000000   0.048308   0.099415      -0.015259  -0.037885   \n",
       "ps_ind_03       0.048308   1.000000   0.002348       0.053250   0.170449   \n",
       "ps_reg_03       0.099415   0.002348   1.000000      -0.019510   0.001956   \n",
       "ps_ind_05_cat  -0.015259   0.053250  -0.019510       1.000000   0.012326   \n",
       "ps_ind_15      -0.037885   0.170449   0.001956       0.012326   1.000000   \n",
       "ps_ind_17_bin   0.055626   0.027691   0.059237       0.007163  -0.021884   \n",
       "reg03/reg02     0.008900  -0.011499   0.271879       0.000990   0.030108   \n",
       "ps_car_14      -0.037310   0.000539   0.006271      -0.001427  -0.005253   \n",
       "ps_reg_02       0.194316   0.048338   0.516457      -0.009124   0.021702   \n",
       "ps_ind_01       0.099752   0.223408   0.139125       0.014823   0.122137   \n",
       "ps_car_01_cat   0.167127  -0.028520   0.230339      -0.036213   0.025690   \n",
       "ps_car_07_cat  -0.223412   0.056337  -0.053356       0.053777   0.059703   \n",
       "ps_reg_01       0.027097   0.003209   0.637035      -0.017836  -0.051826   \n",
       "ps_car_03_cat   0.184690   0.095658   0.054902      -0.011399   0.065398   \n",
       "ps_calc_10      0.001044   0.000230  -0.000138      -0.001070  -0.001279   \n",
       "\n",
       "               ps_ind_17_bin  reg03/reg02  ps_car_14  ps_reg_02  ps_ind_01  \\\n",
       "ps_car_13           0.055626     0.008900  -0.037310   0.194316   0.099752   \n",
       "ps_ind_03           0.027691    -0.011499   0.000539   0.048338   0.223408   \n",
       "ps_reg_03           0.059237     0.271879   0.006271   0.516457   0.139125   \n",
       "ps_ind_05_cat       0.007163     0.000990  -0.001427  -0.009124   0.014823   \n",
       "ps_ind_15          -0.021884     0.030108  -0.005253   0.021702   0.122137   \n",
       "ps_ind_17_bin       1.000000    -0.004817  -0.012650   0.033383   0.101257   \n",
       "reg03/reg02        -0.004817     1.000000  -0.003384  -0.242790  -0.003305   \n",
       "ps_car_14          -0.012650    -0.003384   1.000000   0.004147   0.034480   \n",
       "ps_reg_02           0.033383    -0.242790   0.004147   1.000000   0.183855   \n",
       "ps_ind_01           0.101257    -0.003305   0.034480   0.183855   1.000000   \n",
       "ps_car_01_cat       0.058876     0.003789  -0.027170   0.231710  -0.056121   \n",
       "ps_car_07_cat      -0.043662     0.005216   0.020895  -0.069651  -0.000639   \n",
       "ps_reg_01           0.069967    -0.161803   0.012704   0.471027   0.102212   \n",
       "ps_car_03_cat       0.137328    -0.003786  -0.163385   0.104317   0.043756   \n",
       "ps_calc_10         -0.000699     0.003034  -0.002534   0.000190  -0.001345   \n",
       "\n",
       "               ps_car_01_cat  ps_car_07_cat  ps_reg_01  ps_car_03_cat  \\\n",
       "ps_car_13           0.167127      -0.223412   0.027097       0.184690   \n",
       "ps_ind_03          -0.028520       0.056337   0.003209       0.095658   \n",
       "ps_reg_03           0.230339      -0.053356   0.637035       0.054902   \n",
       "ps_ind_05_cat      -0.036213       0.053777  -0.017836      -0.011399   \n",
       "ps_ind_15           0.025690       0.059703  -0.051826       0.065398   \n",
       "ps_ind_17_bin       0.058876      -0.043662   0.069967       0.137328   \n",
       "reg03/reg02         0.003789       0.005216  -0.161803      -0.003786   \n",
       "ps_car_14          -0.027170       0.020895   0.012704      -0.163385   \n",
       "ps_reg_02           0.231710      -0.069651   0.471027       0.104317   \n",
       "ps_ind_01          -0.056121      -0.000639   0.102212       0.043756   \n",
       "ps_car_01_cat       1.000000       0.001071   0.184266       0.138055   \n",
       "ps_car_07_cat       0.001071       1.000000  -0.045541      -0.148339   \n",
       "ps_reg_01           0.184266      -0.045541   1.000000       0.024797   \n",
       "ps_car_03_cat       0.138055      -0.148339   0.024797       1.000000   \n",
       "ps_calc_10          0.001443      -0.001270  -0.001007       0.001210   \n",
       "\n",
       "               ps_calc_10  \n",
       "ps_car_13        0.001044  \n",
       "ps_ind_03        0.000230  \n",
       "ps_reg_03       -0.000138  \n",
       "ps_ind_05_cat   -0.001070  \n",
       "ps_ind_15       -0.001279  \n",
       "ps_ind_17_bin   -0.000699  \n",
       "reg03/reg02      0.003034  \n",
       "ps_car_14       -0.002534  \n",
       "ps_reg_02        0.000190  \n",
       "ps_ind_01       -0.001345  \n",
       "ps_car_01_cat    0.001443  \n",
       "ps_car_07_cat   -0.001270  \n",
       "ps_reg_01       -0.001007  \n",
       "ps_car_03_cat    0.001210  \n",
       "ps_calc_10       1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features_names = feature_importance_df.head(15)['feature'].tolist()\n",
    "correlation_matrix = train[top_features_names].corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77f7553-c90e-4c45-8925-6b2a70ce5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target           1.000000\n",
      "ps_car_13        0.053899\n",
      "ps_car_12        0.038790\n",
      "ps_ind_17_bin    0.037053\n",
      "ps_car_07_cat    0.036395\n",
      "ps_reg_02        0.034800\n",
      "ps_ind_07_bin    0.034218\n",
      "ps_ind_06_bin    0.034017\n",
      "ps_car_04_cat    0.032900\n",
      "ps_car_03_cat    0.032401\n",
      "ps_car_02_cat    0.031534\n",
      "ps_reg_03        0.030888\n",
      "ps_ind_05_cat    0.029165\n",
      "ps_ind_16_bin    0.027778\n",
      "ps_car_15        0.027667\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 타겟 변수와의 관계\n",
    "corr_with_target = train.corr()['target'].abs().sort_values(ascending=False)\n",
    "# .abs(): 절댓값 제시, 타겟 예측에는 절댓값이 중요 \n",
    "print(corr_with_target.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0814e95-8797-426b-9ca6-3113de0a2d34",
   "metadata": {},
   "source": [
    "## Insights\n",
    "1. reg_03/02/01은 서로 상관관계가 높음 그러나 reg_03은 importance가 굉장히 높은데 01/02는 그렇지 않음 하지만 target과의 corr값에서 02가 제일 높고 오히려 03이 그보다 낮으며, 01은 상위에 포함되지도 않음\n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e64ea0e-56d7-47ae-8ff5-b1283ec7d889",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1621\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "reg03 * reg02: 0.27249 (-0.00191)\n",
      "Drop reg03 * reg02\n",
      "-----------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012324 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1621\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "reg03 / reg02: 0.27576 (+0.00136)\n",
      "Keep reg03 / reg02\n",
      "-----------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1621\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1620\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1622\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 58\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "reg03 - reg02: 0.27443 (+0.00003)\n",
      "Keep reg03 - reg02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_feature_combination(new_name, feat1, feat2, operation='*'):\n",
    "    train_copy = train_df.drop(columns=['id']).copy()\n",
    "    \n",
    "    if operation == '*':\n",
    "        train_copy[new_name] = train_copy[feat1] * train_copy[feat2]\n",
    "    elif operation == '/':\n",
    "        train_copy[new_name] = train_copy[feat1] / (train_copy[feat2] + 1e-8)\n",
    "    elif operation == '-':\n",
    "        train_copy[new_name] = train_copy[feat1] - train_copy[feat2]\n",
    "\n",
    "    \n",
    "    score = quick_cv_test(train_copy, new_name)\n",
    "    \n",
    "    if score > 0.27440:\n",
    "        print(f'Keep {new_name}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'Drop {new_name}')\n",
    "        train_copy = train_copy.drop(columns=[new_name])\n",
    "        return False\n",
    "\n",
    "test_feature_combination('reg03 * reg02', 'ps_reg_03', 'ps_reg_02', operation='*')\n",
    "print('-----------------------------')\n",
    "test_feature_combination('reg03 / reg02', 'ps_reg_03', 'ps_reg_02', operation='/')\n",
    "print('-----------------------------')\n",
    "test_feature_combination('reg03 - reg02', 'ps_reg_03', 'ps_reg_02', operation='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c8b7e13-6601-42f8-b5bb-03d0f81e67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train.columns.tolist()\n",
    "\n",
    "calc_cols = []\n",
    "calc_bin_cols = []\n",
    "for col in train_cols:\n",
    "    if 'calc' in col:\n",
    "        if 'bin' in col:\n",
    "            calc_bin_cols.append(col)\n",
    "        else: \n",
    "            calc_cols.append(col)\n",
    "            \n",
    "ind_cols = []\n",
    "ind_bin_cols = []\n",
    "ind_cat_cols = []\n",
    "\n",
    "for col in train_cols:\n",
    "    if 'ind' in col:\n",
    "        if 'bin' in col:\n",
    "            ind_bin_cols.append(col)\n",
    "        if 'cat' in col:\n",
    "            ind_cat_cols.append(col)\n",
    "        else:\n",
    "            ind_cols.append(col)\n",
    "            \n",
    "reg_cols = []\n",
    "for col in train_cols:\n",
    "    if 'reg' in col:\n",
    "        reg_cols.append(col)\n",
    "            \n",
    "car_cols = []\n",
    "car_cat_cols = []\n",
    "for col in train_cols:\n",
    "    if 'car' in col:\n",
    "        if 'cat' in col:\n",
    "            car_cat_cols.append(col)\n",
    "        else: \n",
    "            car_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b351faa9-d916-467b-8c19-28595de08acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_calc_cols = calc_cols + calc_bin_cols\n",
    "all_ind_cols = ind_cols + ind_bin_cols + ind_cat_cols\n",
    "all_reg_cols = reg_cols\n",
    "all_car_cols = car_cols + car_cat_cols\n",
    "\n",
    "top_calc = ['ps_calc_10']\n",
    "rest_calc = [col for col in all_calc_cols if col not in top_calc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc20fa98-b83b-4493-a239-cf69a3e3be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_df.drop(columns='id').copy()\n",
    "train_test['reg03/reg02'] = train_test['ps_reg_03'] / (train_test['ps_reg_02'] + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13f76466-5e8e-45b5-b33d-18ff9489b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_test.drop(columns=rest_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69fcad5c-6803-44f7-8de6-a1e8e43e1add",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1457\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1460\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1458\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "rest_calc_remove: 0.27824 (+0.00248)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.2782416401227291)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_cv_test(train_test, 'rest_calc_remove', baseline_score=0.27576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e25734b-0e6a-46e2-baa8-102910d6fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_df.drop(columns='id').copy()\n",
    "train_test['reg03/reg02'] = train_test['ps_reg_03'] / (train_test['ps_reg_02'] + 1e-8)\n",
    "train_test = train_test.drop(columns=rest_calc)\n",
    "train_test_check = train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7eb0fde4-b0d8-4a3d-a4f3-dc9e808b353a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1457\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1460\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1458\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "low_features_remove: 0.27824 (+0.00000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.2782416401227291)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_cv_test(train_test, 'low_features_remove', baseline_score=0.27824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baf70aa0-0ca1-45a3-a6ba-81a6f44b8de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_13_bin', 'ps_ind_10_bin', 'ps_ind_11_bin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_features['feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c8bc9509-2d09-4ddb-8d51-0144fe194d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_df.drop(columns='id').copy()\n",
    "train_test['reg03/reg02'] = train_test['ps_reg_03'] / (train_test['ps_reg_02'] + 1e-8)\n",
    "train_test['car02cat/car07cat'] = train_test['ps_car_02_cat'] / (train_test['ps_car_07_cat'] + 1e-8)\n",
    "train_test = train_test.drop(columns=rest_calc)\n",
    "\n",
    "# feature 추가 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4db5f5cb-4d56-4faf-8722-abc3e94039c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1466\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1466\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.27882420684276443)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_cv_test(train_test, 'test', baseline_score=0.27882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7feeb4a-fde9-4bd2-b463-619d290cb032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>reg03/reg02</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_calc_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ps_car_13</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048308</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>-0.015259</td>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>-0.037310</td>\n",
       "      <td>0.194316</td>\n",
       "      <td>0.099752</td>\n",
       "      <td>0.167127</td>\n",
       "      <td>-0.223412</td>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.184690</td>\n",
       "      <td>0.001044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_03</th>\n",
       "      <td>0.048308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>0.170449</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>0.056337</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_03</th>\n",
       "      <td>0.099415</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>0.271879</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>0.139125</td>\n",
       "      <td>0.230339</td>\n",
       "      <td>-0.053356</td>\n",
       "      <td>0.637035</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>-0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>-0.019510</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>-0.036213</td>\n",
       "      <td>0.053777</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>-0.001070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_15</th>\n",
       "      <td>-0.037885</td>\n",
       "      <td>0.170449</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.012326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.059703</td>\n",
       "      <td>-0.051826</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>-0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>-0.021884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>0.101257</td>\n",
       "      <td>0.058876</td>\n",
       "      <td>-0.043662</td>\n",
       "      <td>0.069967</td>\n",
       "      <td>0.137328</td>\n",
       "      <td>-0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg03/reg02</th>\n",
       "      <td>0.008900</td>\n",
       "      <td>-0.011499</td>\n",
       "      <td>0.271879</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.030108</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>-0.161803</td>\n",
       "      <td>-0.003786</td>\n",
       "      <td>0.003034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_14</th>\n",
       "      <td>-0.037310</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>-0.163385</td>\n",
       "      <td>-0.002534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_02</th>\n",
       "      <td>0.194316</td>\n",
       "      <td>0.048338</td>\n",
       "      <td>0.516457</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>0.033383</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.183855</td>\n",
       "      <td>0.231710</td>\n",
       "      <td>-0.069651</td>\n",
       "      <td>0.471027</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_01</th>\n",
       "      <td>0.099752</td>\n",
       "      <td>0.223408</td>\n",
       "      <td>0.139125</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>0.101257</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.183855</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056121</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>-0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <td>0.167127</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>0.230339</td>\n",
       "      <td>-0.036213</td>\n",
       "      <td>0.025690</td>\n",
       "      <td>0.058876</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>-0.027170</td>\n",
       "      <td>0.231710</td>\n",
       "      <td>-0.056121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.184266</td>\n",
       "      <td>0.138055</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <td>-0.223412</td>\n",
       "      <td>0.056337</td>\n",
       "      <td>-0.053356</td>\n",
       "      <td>0.053777</td>\n",
       "      <td>0.059703</td>\n",
       "      <td>-0.043662</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>-0.069651</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045541</td>\n",
       "      <td>-0.148339</td>\n",
       "      <td>-0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_01</th>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.637035</td>\n",
       "      <td>-0.017836</td>\n",
       "      <td>-0.051826</td>\n",
       "      <td>0.069967</td>\n",
       "      <td>-0.161803</td>\n",
       "      <td>0.012704</td>\n",
       "      <td>0.471027</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.184266</td>\n",
       "      <td>-0.045541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>-0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <td>0.184690</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>0.137328</td>\n",
       "      <td>-0.003786</td>\n",
       "      <td>-0.163385</td>\n",
       "      <td>0.104317</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.138055</td>\n",
       "      <td>-0.148339</td>\n",
       "      <td>0.024797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_10</th>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ps_car_13  ps_ind_03  ps_reg_03  ps_ind_05_cat  ps_ind_15  \\\n",
       "ps_car_13       1.000000   0.048308   0.099415      -0.015259  -0.037885   \n",
       "ps_ind_03       0.048308   1.000000   0.002348       0.053250   0.170449   \n",
       "ps_reg_03       0.099415   0.002348   1.000000      -0.019510   0.001956   \n",
       "ps_ind_05_cat  -0.015259   0.053250  -0.019510       1.000000   0.012326   \n",
       "ps_ind_15      -0.037885   0.170449   0.001956       0.012326   1.000000   \n",
       "ps_ind_17_bin   0.055626   0.027691   0.059237       0.007163  -0.021884   \n",
       "reg03/reg02     0.008900  -0.011499   0.271879       0.000990   0.030108   \n",
       "ps_car_14      -0.037310   0.000539   0.006271      -0.001427  -0.005253   \n",
       "ps_reg_02       0.194316   0.048338   0.516457      -0.009124   0.021702   \n",
       "ps_ind_01       0.099752   0.223408   0.139125       0.014823   0.122137   \n",
       "ps_car_01_cat   0.167127  -0.028520   0.230339      -0.036213   0.025690   \n",
       "ps_car_07_cat  -0.223412   0.056337  -0.053356       0.053777   0.059703   \n",
       "ps_reg_01       0.027097   0.003209   0.637035      -0.017836  -0.051826   \n",
       "ps_car_03_cat   0.184690   0.095658   0.054902      -0.011399   0.065398   \n",
       "ps_calc_10      0.001044   0.000230  -0.000138      -0.001070  -0.001279   \n",
       "\n",
       "               ps_ind_17_bin  reg03/reg02  ps_car_14  ps_reg_02  ps_ind_01  \\\n",
       "ps_car_13           0.055626     0.008900  -0.037310   0.194316   0.099752   \n",
       "ps_ind_03           0.027691    -0.011499   0.000539   0.048338   0.223408   \n",
       "ps_reg_03           0.059237     0.271879   0.006271   0.516457   0.139125   \n",
       "ps_ind_05_cat       0.007163     0.000990  -0.001427  -0.009124   0.014823   \n",
       "ps_ind_15          -0.021884     0.030108  -0.005253   0.021702   0.122137   \n",
       "ps_ind_17_bin       1.000000    -0.004817  -0.012650   0.033383   0.101257   \n",
       "reg03/reg02        -0.004817     1.000000  -0.003384  -0.242790  -0.003305   \n",
       "ps_car_14          -0.012650    -0.003384   1.000000   0.004147   0.034480   \n",
       "ps_reg_02           0.033383    -0.242790   0.004147   1.000000   0.183855   \n",
       "ps_ind_01           0.101257    -0.003305   0.034480   0.183855   1.000000   \n",
       "ps_car_01_cat       0.058876     0.003789  -0.027170   0.231710  -0.056121   \n",
       "ps_car_07_cat      -0.043662     0.005216   0.020895  -0.069651  -0.000639   \n",
       "ps_reg_01           0.069967    -0.161803   0.012704   0.471027   0.102212   \n",
       "ps_car_03_cat       0.137328    -0.003786  -0.163385   0.104317   0.043756   \n",
       "ps_calc_10         -0.000699     0.003034  -0.002534   0.000190  -0.001345   \n",
       "\n",
       "               ps_car_01_cat  ps_car_07_cat  ps_reg_01  ps_car_03_cat  \\\n",
       "ps_car_13           0.167127      -0.223412   0.027097       0.184690   \n",
       "ps_ind_03          -0.028520       0.056337   0.003209       0.095658   \n",
       "ps_reg_03           0.230339      -0.053356   0.637035       0.054902   \n",
       "ps_ind_05_cat      -0.036213       0.053777  -0.017836      -0.011399   \n",
       "ps_ind_15           0.025690       0.059703  -0.051826       0.065398   \n",
       "ps_ind_17_bin       0.058876      -0.043662   0.069967       0.137328   \n",
       "reg03/reg02         0.003789       0.005216  -0.161803      -0.003786   \n",
       "ps_car_14          -0.027170       0.020895   0.012704      -0.163385   \n",
       "ps_reg_02           0.231710      -0.069651   0.471027       0.104317   \n",
       "ps_ind_01          -0.056121      -0.000639   0.102212       0.043756   \n",
       "ps_car_01_cat       1.000000       0.001071   0.184266       0.138055   \n",
       "ps_car_07_cat       0.001071       1.000000  -0.045541      -0.148339   \n",
       "ps_reg_01           0.184266      -0.045541   1.000000       0.024797   \n",
       "ps_car_03_cat       0.138055      -0.148339   0.024797       1.000000   \n",
       "ps_calc_10          0.001443      -0.001270  -0.001007       0.001210   \n",
       "\n",
       "               ps_calc_10  \n",
       "ps_car_13        0.001044  \n",
       "ps_ind_03        0.000230  \n",
       "ps_reg_03       -0.000138  \n",
       "ps_ind_05_cat   -0.001070  \n",
       "ps_ind_15       -0.001279  \n",
       "ps_ind_17_bin   -0.000699  \n",
       "reg03/reg02      0.003034  \n",
       "ps_car_14       -0.002534  \n",
       "ps_reg_02        0.000190  \n",
       "ps_ind_01       -0.001345  \n",
       "ps_car_01_cat    0.001443  \n",
       "ps_car_07_cat   -0.001270  \n",
       "ps_reg_01       -0.001007  \n",
       "ps_car_03_cat    0.001210  \n",
       "ps_calc_10       1.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "136cd5bd-4a67-4c97-a84d-e3d71995a475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ps_car_13</td>\n",
       "      <td>7949.388527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ps_ind_03</td>\n",
       "      <td>3398.219804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ps_reg_03</td>\n",
       "      <td>3372.933639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ps_ind_05_cat</td>\n",
       "      <td>3223.870274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ps_ind_15</td>\n",
       "      <td>2678.467430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ps_ind_17_bin</td>\n",
       "      <td>2282.083090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>reg03/reg02</td>\n",
       "      <td>2197.275825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ps_car_14</td>\n",
       "      <td>1738.022992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ps_reg_02</td>\n",
       "      <td>1565.612011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ps_ind_01</td>\n",
       "      <td>1548.323184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ps_car_01_cat</td>\n",
       "      <td>1485.993716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ps_car_07_cat</td>\n",
       "      <td>1469.186490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ps_reg_01</td>\n",
       "      <td>1456.950536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ps_car_03_cat</td>\n",
       "      <td>1305.829952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ps_calc_10</td>\n",
       "      <td>1265.120710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature   importance\n",
       "34      ps_car_13  7949.388527\n",
       "2       ps_ind_03  3398.219804\n",
       "20      ps_reg_03  3372.933639\n",
       "4   ps_ind_05_cat  3223.870274\n",
       "14      ps_ind_15  2678.467430\n",
       "16  ps_ind_17_bin  2282.083090\n",
       "57    reg03/reg02  2197.275825\n",
       "35      ps_car_14  1738.022992\n",
       "19      ps_reg_02  1565.612011\n",
       "0       ps_ind_01  1548.323184\n",
       "21  ps_car_01_cat  1485.993716\n",
       "27  ps_car_07_cat  1469.186490\n",
       "18      ps_reg_01  1456.950536\n",
       "23  ps_car_03_cat  1305.829952\n",
       "46     ps_calc_10  1265.120710"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "521569a6-12c7-4a4a-b33f-9534d7f6f944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_car_13: 0.278824320968703',\n",
       " 'ps_car_12: 0.27882420684276443',\n",
       " 'ps_ind_17_bin: 0.27882420684276443',\n",
       " 'ps_reg_02: 0.27882420684276443',\n",
       " 'ps_ind_07_bin: 0.27882420684276443',\n",
       " 'ps_ind_06_bin: 0.27882420684276443',\n",
       " 'ps_car_04_cat: 0.27882420684276443',\n",
       " 'ps_ind_16_bin: 0.27882420684276443',\n",
       " 'ps_car_15: 0.27882420684276443']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15853548-a0e7-4b62-8d57-8117b2077f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target           1.000000\n",
       "ps_car_13        0.053899\n",
       "ps_car_12        0.038790\n",
       "ps_ind_17_bin    0.037053\n",
       "ps_car_07_cat    0.036395\n",
       "ps_reg_02        0.034800\n",
       "ps_ind_07_bin    0.034218\n",
       "ps_ind_06_bin    0.034017\n",
       "ps_car_04_cat    0.032900\n",
       "ps_car_03_cat    0.032401\n",
       "ps_car_02_cat    0.031534\n",
       "ps_reg_03        0.030888\n",
       "ps_ind_05_cat    0.029165\n",
       "ps_ind_16_bin    0.027778\n",
       "ps_car_15        0.027667\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_with_target.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebeff4a3-8ace-465e-9e1e-e64d680dd32b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1720\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1721\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009328 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1721\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1585\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1590\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1593\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27764 (-0.00118)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1480\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1484\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009563 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1485\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1482\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008822 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1485\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1471\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1475\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1476\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1476\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010668 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27751 (-0.00131)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009524 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009559 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27868 (-0.00014)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009351 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1716\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1720\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1721\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1718\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009359 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1721\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27603 (-0.00279)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1472\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1470\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27709 (-0.00173)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1463\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1465\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008824 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1468\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1476\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1480\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1481\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1478\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1481\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 41\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27882 (+0.00000)\n"
     ]
    }
   ],
   "source": [
    "good_features = []\n",
    "top = corr_with_target.head(15).index.tolist()\n",
    "top.remove('target')\n",
    "for features in top:\n",
    "    train_test = train_df.drop(columns='id').copy()\n",
    "    train_test['reg03/reg02'] = train_test['ps_reg_03'] / (train_test['ps_reg_02'] + 1e-8)\n",
    "    train_test['car02cat/car07cat'] = train_test['ps_car_02_cat'] / (train_test['ps_car_07_cat'] + 1e-8)\n",
    "    train_test = train_test.drop(columns=rest_calc)\n",
    "\n",
    "    # feature 추가 \n",
    "    # train_test['test'] = train_test['ps_car_12'] / (train_test[f'{features}'] + 1e-8)\n",
    "    train_test['test'] = train_test[f'{features}'] ** 2\n",
    "    \n",
    "    score = quick_cv_test(train_test, 'test', baseline_score=0.27882)\n",
    "    if score > 0.27882:\n",
    "        good_features.append(f'{features}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d5ed7321-df2b-4e72-b988-b2b93f71b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train_df.drop(columns='id').copy()\n",
    "train_test['reg03/reg02'] = train_test['ps_reg_03'] / (train_test['ps_reg_02'] + 1e-8)\n",
    "train_test = train_test.drop(columns=rest_calc)\n",
    "train_test_check = train_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1453f4ce-0d04-42f3-a8c3-846777a05068",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mean = train_test.groupby('ps_ind_02_cat')['ps_car_13'].mean()\n",
    "train['car13_mean_by_ind02'] = train['ps_ind_02_cat'].map(group_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "00788236-c564-4da0-a86f-c15adf9aa715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1457\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1460\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1458\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1461\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "test: 0.27824 (-0.00058)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.2782416401227291)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_cv_test(train_test, 'test', baseline_score=0.27882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6295fe61-e73d-49ad-a690-0a64c42de9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DriverPred/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-27 16:18:29,757] A new study created in memory with name: no-name-1d831149-7b48-4a93-991e-0a77fc9fd991\n",
      "[I 2025-06-27 16:19:14,574] Trial 0 finished with value: 0.2788103880402309 and parameters: {'n_estimators': 1821, 'learning_rate': 0.024177891965198043, 'max_depth': 4, 'num_leaves': 83, 'min_child_samples': 109, 'subsample': 0.8869006494743602, 'colsample_bytree': 0.8496920451626392, 'reg_alpha': 0.2527559287819038, 'reg_lambda': 0.045953980818960835}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:21:17,872] Trial 1 finished with value: 0.18144176196993872 and parameters: {'n_estimators': 1672, 'learning_rate': 0.07662129557451254, 'max_depth': 7, 'num_leaves': 100, 'min_child_samples': 81, 'subsample': 0.841259514212409, 'colsample_bytree': 0.8339733573032178, 'reg_alpha': 0.033194499658650886, 'reg_lambda': 0.10566222640610434}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:22:18,909] Trial 2 finished with value: 0.26279559129825253 and parameters: {'n_estimators': 1772, 'learning_rate': 0.0576656104465518, 'max_depth': 7, 'num_leaves': 25, 'min_child_samples': 55, 'subsample': 0.802699913618184, 'colsample_bytree': 0.7295959861365087, 'reg_alpha': 4.238792855241178, 'reg_lambda': 4.2424911463762935}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:23:10,676] Trial 3 finished with value: 0.2503486933048723 and parameters: {'n_estimators': 1259, 'learning_rate': 0.07665863504464385, 'max_depth': 7, 'num_leaves': 34, 'min_child_samples': 172, 'subsample': 0.7541908159193473, 'colsample_bytree': 0.78827033010231, 'reg_alpha': 2.7986857568123042, 'reg_lambda': 0.07966267617710358}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:25:15,304] Trial 4 finished with value: 0.24762871870158337 and parameters: {'n_estimators': 1823, 'learning_rate': 0.027877654804285154, 'max_depth': 7, 'num_leaves': 97, 'min_child_samples': 129, 'subsample': 0.8133171463052551, 'colsample_bytree': 0.7677849446785141, 'reg_alpha': 0.05285259314709262, 'reg_lambda': 0.7906954114801201}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:26:48,451] Trial 5 finished with value: 0.23847593563682393 and parameters: {'n_estimators': 1617, 'learning_rate': 0.050244545007785, 'max_depth': 7, 'num_leaves': 54, 'min_child_samples': 151, 'subsample': 0.8607606656250127, 'colsample_bytree': 0.7578024546558537, 'reg_alpha': 1.8322865209258445, 'reg_lambda': 0.060292416125672134}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:28:17,025] Trial 6 finished with value: 0.2174811377602323 and parameters: {'n_estimators': 1883, 'learning_rate': 0.07068970714253196, 'max_depth': 8, 'num_leaves': 43, 'min_child_samples': 88, 'subsample': 0.8270974622924936, 'colsample_bytree': 0.794533185970561, 'reg_alpha': 0.12644639120419895, 'reg_lambda': 0.10921583537149601}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:29:13,828] Trial 7 finished with value: 0.23961545825071634 and parameters: {'n_estimators': 1767, 'learning_rate': 0.07390699651341985, 'max_depth': 5, 'num_leaves': 92, 'min_child_samples': 195, 'subsample': 0.7310416507016528, 'colsample_bytree': 0.7236292149353215, 'reg_alpha': 0.07387816907936533, 'reg_lambda': 0.08961905066920912}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:30:52,809] Trial 8 finished with value: 0.24214679550862112 and parameters: {'n_estimators': 1997, 'learning_rate': 0.045270187329564476, 'max_depth': 6, 'num_leaves': 58, 'min_child_samples': 74, 'subsample': 0.7113772253826617, 'colsample_bytree': 0.831640304172289, 'reg_alpha': 0.018153499901451185, 'reg_lambda': 4.651024047105217}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:31:48,239] Trial 9 finished with value: 0.2421011125634962 and parameters: {'n_estimators': 1065, 'learning_rate': 0.06951075136934985, 'max_depth': 6, 'num_leaves': 88, 'min_child_samples': 134, 'subsample': 0.809480369498263, 'colsample_bytree': 0.7003662539169848, 'reg_alpha': 1.0888202008227372, 'reg_lambda': 0.044226326594651574}. Best is trial 0 with value: 0.2788103880402309.\n",
      "[I 2025-06-27 16:32:26,927] Trial 10 finished with value: 0.283391596698858 and parameters: {'n_estimators': 1514, 'learning_rate': 0.01337314219011241, 'max_depth': 4, 'num_leaves': 76, 'min_child_samples': 105, 'subsample': 0.8986597482504192, 'colsample_bytree': 0.899818132325333, 'reg_alpha': 0.3863416070588609, 'reg_lambda': 0.013404020315404382}. Best is trial 10 with value: 0.283391596698858.\n",
      "[I 2025-06-27 16:33:02,698] Trial 11 finished with value: 0.2833125947451624 and parameters: {'n_estimators': 1421, 'learning_rate': 0.010995538923271118, 'max_depth': 4, 'num_leaves': 76, 'min_child_samples': 105, 'subsample': 0.89943043730908, 'colsample_bytree': 0.8971928107242869, 'reg_alpha': 0.4365723212131461, 'reg_lambda': 0.01424056178284482}. Best is trial 10 with value: 0.283391596698858.\n",
      "[I 2025-06-27 16:33:38,352] Trial 12 finished with value: 0.28339779098631296 and parameters: {'n_estimators': 1425, 'learning_rate': 0.013319285404006688, 'max_depth': 4, 'num_leaves': 72, 'min_child_samples': 106, 'subsample': 0.8898041305740427, 'colsample_bytree': 0.8934668786404972, 'reg_alpha': 0.6174472853227995, 'reg_lambda': 0.011067635421984781}. Best is trial 12 with value: 0.28339779098631296.\n",
      "[I 2025-06-27 16:34:34,748] Trial 13 finished with value: 0.2837119490551242 and parameters: {'n_estimators': 1453, 'learning_rate': 0.010708055199920595, 'max_depth': 5, 'num_leaves': 72, 'min_child_samples': 104, 'subsample': 0.8678928538691494, 'colsample_bytree': 0.896174677824912, 'reg_alpha': 8.913175004514418, 'reg_lambda': 0.010026126574242487}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:35:26,812] Trial 14 finished with value: 0.2766533314362258 and parameters: {'n_estimators': 1351, 'learning_rate': 0.0289902772402156, 'max_depth': 5, 'num_leaves': 67, 'min_child_samples': 150, 'subsample': 0.8591512346684573, 'colsample_bytree': 0.8666826193309737, 'reg_alpha': 9.448988577550795, 'reg_lambda': 0.4135581989735653}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:36:12,518] Trial 15 finished with value: 0.2809063706848086 and parameters: {'n_estimators': 1195, 'learning_rate': 0.021067607126989768, 'max_depth': 5, 'num_leaves': 69, 'min_child_samples': 66, 'subsample': 0.7674626614306311, 'colsample_bytree': 0.8723549025739133, 'reg_alpha': 8.419727666040947, 'reg_lambda': 0.013465888851739629}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:37:05,705] Trial 16 finished with value: 0.264979330042668 and parameters: {'n_estimators': 1524, 'learning_rate': 0.03760487937287642, 'max_depth': 5, 'num_leaves': 51, 'min_child_samples': 97, 'subsample': 0.8713768877339901, 'colsample_bytree': 0.8744733074875937, 'reg_alpha': 0.8225262104080995, 'reg_lambda': 0.023969960895480796}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:37:40,334] Trial 17 finished with value: 0.2836051148312215 and parameters: {'n_estimators': 1359, 'learning_rate': 0.0184319085449958, 'max_depth': 4, 'num_leaves': 67, 'min_child_samples': 119, 'subsample': 0.8365430546921478, 'colsample_bytree': 0.8322940558987207, 'reg_alpha': 4.729399134983957, 'reg_lambda': 1.6624007182240397}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:38:18,217] Trial 18 finished with value: 0.2774258862755155 and parameters: {'n_estimators': 1002, 'learning_rate': 0.03620051029357147, 'max_depth': 5, 'num_leaves': 46, 'min_child_samples': 124, 'subsample': 0.7827948403821449, 'colsample_bytree': 0.8322053696149956, 'reg_alpha': 4.181962513801424, 'reg_lambda': 1.5923206290833123}. Best is trial 13 with value: 0.2837119490551242.\n",
      "[I 2025-06-27 16:38:50,250] Trial 19 finished with value: 0.2838448018017648 and parameters: {'n_estimators': 1275, 'learning_rate': 0.01928623355655875, 'max_depth': 4, 'num_leaves': 63, 'min_child_samples': 147, 'subsample': 0.8401364280113609, 'colsample_bytree': 0.8185656743563989, 'reg_alpha': 1.534543692336667, 'reg_lambda': 1.4963368018314083}. Best is trial 19 with value: 0.2838448018017648.\n",
      "[I 2025-06-27 16:39:51,493] Trial 20 finished with value: 0.26975386544308144 and parameters: {'n_estimators': 1175, 'learning_rate': 0.0344714955624988, 'max_depth': 6, 'num_leaves': 60, 'min_child_samples': 152, 'subsample': 0.8491655383252585, 'colsample_bytree': 0.8114925673377938, 'reg_alpha': 1.595614482776744, 'reg_lambda': 8.894358639169964}. Best is trial 19 with value: 0.2838448018017648.\n",
      "[I 2025-06-27 16:40:25,175] Trial 21 finished with value: 0.2840297702373386 and parameters: {'n_estimators': 1314, 'learning_rate': 0.019002926362005984, 'max_depth': 4, 'num_leaves': 63, 'min_child_samples': 123, 'subsample': 0.8340195277389157, 'colsample_bytree': 0.8094510148477003, 'reg_alpha': 4.553992521224537, 'reg_lambda': 1.5590590523646795}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:40:58,594] Trial 22 finished with value: 0.2837762713154827 and parameters: {'n_estimators': 1285, 'learning_rate': 0.017939111892640306, 'max_depth': 4, 'num_leaves': 61, 'min_child_samples': 140, 'subsample': 0.8717811381756634, 'colsample_bytree': 0.8117567033387562, 'reg_alpha': 2.5399061657870488, 'reg_lambda': 0.2494524050947696}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:41:32,485] Trial 23 finished with value: 0.2835543811006157 and parameters: {'n_estimators': 1308, 'learning_rate': 0.020678307245883588, 'max_depth': 4, 'num_leaves': 61, 'min_child_samples': 169, 'subsample': 0.821099234320528, 'colsample_bytree': 0.8123200944244434, 'reg_alpha': 2.1241737378042775, 'reg_lambda': 0.259527488698287}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:42:01,084] Trial 24 finished with value: 0.28228943158262304 and parameters: {'n_estimators': 1143, 'learning_rate': 0.030169063033642328, 'max_depth': 4, 'num_leaves': 41, 'min_child_samples': 139, 'subsample': 0.8452845569999667, 'colsample_bytree': 0.7857607280359312, 'reg_alpha': 1.0725949600607985, 'reg_lambda': 0.23620567505365117}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:42:33,750] Trial 25 finished with value: 0.2837743651584238 and parameters: {'n_estimators': 1239, 'learning_rate': 0.0161213410131993, 'max_depth': 4, 'num_leaves': 51, 'min_child_samples': 168, 'subsample': 0.7894950955350011, 'colsample_bytree': 0.8070914184295134, 'reg_alpha': 2.88472530196685, 'reg_lambda': 0.8828114968386751}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:43:13,357] Trial 26 finished with value: 0.2796955580379684 and parameters: {'n_estimators': 1102, 'learning_rate': 0.025063203112347845, 'max_depth': 5, 'num_leaves': 82, 'min_child_samples': 142, 'subsample': 0.8782134072032547, 'colsample_bytree': 0.7703370668073138, 'reg_alpha': 0.1900952337418394, 'reg_lambda': 0.4724431407628271}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:43:46,373] Trial 27 finished with value: 0.27878030553587047 and parameters: {'n_estimators': 1282, 'learning_rate': 0.04100512448603581, 'max_depth': 4, 'num_leaves': 63, 'min_child_samples': 183, 'subsample': 0.8276031998505141, 'colsample_bytree': 0.8437216156681796, 'reg_alpha': 4.7030130386049676, 'reg_lambda': 2.028472161726632}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:44:57,413] Trial 28 finished with value: 0.23836587198064407 and parameters: {'n_estimators': 1360, 'learning_rate': 0.057719349209043654, 'max_depth': 6, 'num_leaves': 55, 'min_child_samples': 119, 'subsample': 0.8520283738633915, 'colsample_bytree': 0.8165599788637622, 'reg_alpha': 1.2105351491939107, 'reg_lambda': 0.17169638681621852}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:45:35,318] Trial 29 finished with value: 0.28109778228379034 and parameters: {'n_estimators': 1560, 'learning_rate': 0.02399325324585528, 'max_depth': 4, 'num_leaves': 82, 'min_child_samples': 154, 'subsample': 0.8778766894648246, 'colsample_bytree': 0.8535299813024302, 'reg_alpha': 0.515355472869846, 'reg_lambda': 0.8082627011666477}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:46:29,836] Trial 30 finished with value: 0.2766121644161714 and parameters: {'n_estimators': 1212, 'learning_rate': 0.03199532671704884, 'max_depth': 8, 'num_leaves': 34, 'min_child_samples': 162, 'subsample': 0.7903372506346745, 'colsample_bytree': 0.8190507853936898, 'reg_alpha': 3.205952773260967, 'reg_lambda': 4.251872605427831}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:47:03,109] Trial 31 finished with value: 0.2838981875216436 and parameters: {'n_estimators': 1249, 'learning_rate': 0.017106897224515964, 'max_depth': 4, 'num_leaves': 50, 'min_child_samples': 181, 'subsample': 0.7890791984485328, 'colsample_bytree': 0.8015299099076783, 'reg_alpha': 2.5276557748326014, 'reg_lambda': 0.874636422637083}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:47:35,796] Trial 32 finished with value: 0.2840216609249202 and parameters: {'n_estimators': 1306, 'learning_rate': 0.017472493508416842, 'max_depth': 4, 'num_leaves': 49, 'min_child_samples': 187, 'subsample': 0.7671116903011385, 'colsample_bytree': 0.782488781463978, 'reg_alpha': 1.9099732304977146, 'reg_lambda': 1.0927623250217329}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:48:10,539] Trial 33 finished with value: 0.28334912922307887 and parameters: {'n_estimators': 1334, 'learning_rate': 0.025489474022640464, 'max_depth': 4, 'num_leaves': 46, 'min_child_samples': 197, 'subsample': 0.754311863468536, 'colsample_bytree': 0.771495546128517, 'reg_alpha': 6.161859124656265, 'reg_lambda': 2.443996057509968}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:49:01,622] Trial 34 finished with value: 0.2799600632462059 and parameters: {'n_estimators': 1419, 'learning_rate': 0.021504127154387834, 'max_depth': 5, 'num_leaves': 34, 'min_child_samples': 183, 'subsample': 0.7829718573647453, 'colsample_bytree': 0.7459414937964516, 'reg_alpha': 1.6046778137767794, 'reg_lambda': 1.2043149334371683}. Best is trial 21 with value: 0.2840297702373386.\n",
      "[I 2025-06-27 16:49:31,052] Trial 35 finished with value: 0.2843478805651914 and parameters: {'n_estimators': 1135, 'learning_rate': 0.014940559270391411, 'max_depth': 4, 'num_leaves': 51, 'min_child_samples': 183, 'subsample': 0.7634031002230968, 'colsample_bytree': 0.7831642382223278, 'reg_alpha': 0.2638737371334265, 'reg_lambda': 2.799133541730234}. Best is trial 35 with value: 0.2843478805651914.\n",
      "[I 2025-06-27 16:50:00,108] Trial 36 finished with value: 0.2839491982742942 and parameters: {'n_estimators': 1105, 'learning_rate': 0.015049778087192578, 'max_depth': 4, 'num_leaves': 25, 'min_child_samples': 188, 'subsample': 0.7610483027885074, 'colsample_bytree': 0.7852800792267675, 'reg_alpha': 0.24568789511595224, 'reg_lambda': 0.5869814129528421}. Best is trial 35 with value: 0.2843478805651914.\n",
      "[I 2025-06-27 16:50:32,995] Trial 37 finished with value: 0.28451752460810786 and parameters: {'n_estimators': 1013, 'learning_rate': 0.014311236907790873, 'max_depth': 5, 'num_leaves': 21, 'min_child_samples': 190, 'subsample': 0.7402301402634496, 'colsample_bytree': 0.7834697495638584, 'reg_alpha': 0.230353279355844, 'reg_lambda': 2.934135510295971}. Best is trial 37 with value: 0.28451752460810786.\n",
      "[I 2025-06-27 16:51:07,030] Trial 38 finished with value: 0.2711343755618028 and parameters: {'n_estimators': 1023, 'learning_rate': 0.054972545046766796, 'max_depth': 5, 'num_leaves': 28, 'min_child_samples': 176, 'subsample': 0.7377244722838154, 'colsample_bytree': 0.778809154015214, 'reg_alpha': 0.11451631925843876, 'reg_lambda': 8.265795332192226}. Best is trial 37 with value: 0.28451752460810786.\n",
      "[I 2025-06-27 16:51:45,569] Trial 39 finished with value: 0.2806618692615123 and parameters: {'n_estimators': 1092, 'learning_rate': 0.024703503865223762, 'max_depth': 5, 'num_leaves': 38, 'min_child_samples': 192, 'subsample': 0.7357046099078213, 'colsample_bytree': 0.75475741552106, 'reg_alpha': 0.27540473900806156, 'reg_lambda': 2.9678136063588267}. Best is trial 37 with value: 0.28451752460810786.\n",
      "[I 2025-06-27 16:52:27,558] Trial 40 finished with value: 0.28469721292457006 and parameters: {'n_estimators': 1153, 'learning_rate': 0.01062145654472021, 'max_depth': 5, 'num_leaves': 30, 'min_child_samples': 199, 'subsample': 0.7004899487900638, 'colsample_bytree': 0.7388062912342801, 'reg_alpha': 0.14998718481835707, 'reg_lambda': 2.9179287219198615}. Best is trial 40 with value: 0.28469721292457006.\n",
      "[I 2025-06-27 16:53:16,135] Trial 41 finished with value: 0.2847599645224495 and parameters: {'n_estimators': 1149, 'learning_rate': 0.010571253522122341, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 200, 'subsample': 0.7010568689348484, 'colsample_bytree': 0.7344259741108176, 'reg_alpha': 0.1537921200703024, 'reg_lambda': 5.629696134862305}. Best is trial 41 with value: 0.2847599645224495.\n",
      "[I 2025-06-27 16:53:57,991] Trial 42 finished with value: 0.28490295573624247 and parameters: {'n_estimators': 1148, 'learning_rate': 0.010348087115693351, 'max_depth': 6, 'num_leaves': 21, 'min_child_samples': 197, 'subsample': 0.7011626770578101, 'colsample_bytree': 0.7268711348500145, 'reg_alpha': 0.05593309064492306, 'reg_lambda': 5.564477743593939}. Best is trial 42 with value: 0.28490295573624247.\n",
      "[I 2025-06-27 16:54:35,396] Trial 43 finished with value: 0.28446070185734734 and parameters: {'n_estimators': 1045, 'learning_rate': 0.012020313595127457, 'max_depth': 7, 'num_leaves': 20, 'min_child_samples': 198, 'subsample': 0.7010580193425957, 'colsample_bytree': 0.7246516650501524, 'reg_alpha': 0.039501105212381135, 'reg_lambda': 3.869549326893057}. Best is trial 42 with value: 0.28490295573624247.\n",
      "[I 2025-06-27 16:55:12,068] Trial 44 finished with value: 0.2845586973453308 and parameters: {'n_estimators': 1048, 'learning_rate': 0.010945399187099754, 'max_depth': 7, 'num_leaves': 20, 'min_child_samples': 200, 'subsample': 0.7020805926666569, 'colsample_bytree': 0.7243800559992016, 'reg_alpha': 0.04223644468827434, 'reg_lambda': 6.347763664550371}. Best is trial 42 with value: 0.28490295573624247.\n",
      "[I 2025-06-27 16:55:55,160] Trial 45 finished with value: 0.28530645743654054 and parameters: {'n_estimators': 1053, 'learning_rate': 0.010770491941682607, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 193, 'subsample': 0.7243615184930295, 'colsample_bytree': 0.7074826973768649, 'reg_alpha': 0.01764882291197985, 'reg_lambda': 6.156851511554824}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 16:56:41,075] Trial 46 finished with value: 0.28528801587308317 and parameters: {'n_estimators': 1152, 'learning_rate': 0.01114453592221007, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 198, 'subsample': 0.7197152395415787, 'colsample_bytree': 0.7073616482151218, 'reg_alpha': 0.010996374778750855, 'reg_lambda': 6.227339770805928}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 16:57:24,658] Trial 47 finished with value: 0.2546318646540512 and parameters: {'n_estimators': 1158, 'learning_rate': 0.079763397038665, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 177, 'subsample': 0.7193084914312272, 'colsample_bytree': 0.7026487814878983, 'reg_alpha': 0.011185309966014195, 'reg_lambda': 5.321314295312985}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 16:58:13,907] Trial 48 finished with value: 0.28455461850106817 and parameters: {'n_estimators': 1199, 'learning_rate': 0.010928861265037407, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 162, 'subsample': 0.7211310328306745, 'colsample_bytree': 0.7112357350091807, 'reg_alpha': 0.025152562503456332, 'reg_lambda': 6.697489007079553}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 16:58:54,894] Trial 49 finished with value: 0.284785696876358 and parameters: {'n_estimators': 1117, 'learning_rate': 0.013383376550528278, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 194, 'subsample': 0.7096202021385164, 'colsample_bytree': 0.7383501647549785, 'reg_alpha': 0.07532664984102408, 'reg_lambda': 9.556396891029054}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 16:59:48,500] Trial 50 finished with value: 0.26176497577036917 and parameters: {'n_estimators': 1704, 'learning_rate': 0.06490322183307276, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 190, 'subsample': 0.7151464110469379, 'colsample_bytree': 0.7103030149455575, 'reg_alpha': 0.07041823406997108, 'reg_lambda': 9.377190368685016}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:00:31,132] Trial 51 finished with value: 0.2845405876400545 and parameters: {'n_estimators': 1079, 'learning_rate': 0.010006231934315029, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 199, 'subsample': 0.7069547636523981, 'colsample_bytree': 0.7373805276050343, 'reg_alpha': 0.011606584124216276, 'reg_lambda': 5.987707870966079}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:01:20,142] Trial 52 finished with value: 0.2842969917682632 and parameters: {'n_estimators': 1134, 'learning_rate': 0.013491917703413433, 'max_depth': 6, 'num_leaves': 34, 'min_child_samples': 194, 'subsample': 0.7256039579100719, 'colsample_bytree': 0.7386248382420674, 'reg_alpha': 0.12148602659514735, 'reg_lambda': 3.897915711981742}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:02:14,524] Trial 53 finished with value: 0.2836280652501859 and parameters: {'n_estimators': 1234, 'learning_rate': 0.015039938922253044, 'max_depth': 6, 'num_leaves': 38, 'min_child_samples': 177, 'subsample': 0.7099091380826221, 'colsample_bytree': 0.7127124472858388, 'reg_alpha': 0.019142331546337692, 'reg_lambda': 7.05278425008253}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:02:54,583] Trial 54 finished with value: 0.283052447329292 and parameters: {'n_estimators': 1176, 'learning_rate': 0.022187281777410736, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 194, 'subsample': 0.728654138380517, 'colsample_bytree': 0.7356385933260753, 'reg_alpha': 0.07797803497945544, 'reg_lambda': 4.986502740165861}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:03:39,019] Trial 55 finished with value: 0.2849014879948202 and parameters: {'n_estimators': 1066, 'learning_rate': 0.01336556594347369, 'max_depth': 7, 'num_leaves': 30, 'min_child_samples': 50, 'subsample': 0.7138936051819015, 'colsample_bytree': 0.7161503262059796, 'reg_alpha': 0.1601169804849846, 'reg_lambda': 9.891711561829489}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:04:24,915] Trial 56 finished with value: 0.2847539426916568 and parameters: {'n_estimators': 1053, 'learning_rate': 0.01266165775946241, 'max_depth': 7, 'num_leaves': 32, 'min_child_samples': 84, 'subsample': 0.7144570605687612, 'colsample_bytree': 0.7184983854065171, 'reg_alpha': 0.026738347855958057, 'reg_lambda': 9.76366873230198}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:05:19,253] Trial 57 finished with value: 0.28329076888064997 and parameters: {'n_estimators': 1115, 'learning_rate': 0.017087040922400203, 'max_depth': 8, 'num_leaves': 38, 'min_child_samples': 59, 'subsample': 0.7244731653239413, 'colsample_bytree': 0.7035862471277564, 'reg_alpha': 0.0848425850992443, 'reg_lambda': 7.846196873370132}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:05:54,752] Trial 58 finished with value: 0.2842678889853535 and parameters: {'n_estimators': 1081, 'learning_rate': 0.019462315139970984, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 74, 'subsample': 0.7470723650719809, 'colsample_bytree': 0.7530982008406564, 'reg_alpha': 0.054881542621746764, 'reg_lambda': 5.048765404737738}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:06:40,847] Trial 59 finished with value: 0.2843995264462139 and parameters: {'n_estimators': 1212, 'learning_rate': 0.013788350217784934, 'max_depth': 7, 'num_leaves': 27, 'min_child_samples': 97, 'subsample': 0.7088540929276114, 'colsample_bytree': 0.7276375615150839, 'reg_alpha': 0.019074428117776068, 'reg_lambda': 3.7677123871336486}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:08:11,848] Trial 60 finished with value: 0.26980314543536477 and parameters: {'n_estimators': 1967, 'learning_rate': 0.027198293070795066, 'max_depth': 7, 'num_leaves': 42, 'min_child_samples': 53, 'subsample': 0.7447445424059511, 'colsample_bytree': 0.7164064036973217, 'reg_alpha': 0.014397255230429857, 'reg_lambda': 9.883413867587304}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:08:58,174] Trial 61 finished with value: 0.2848084910353562 and parameters: {'n_estimators': 1050, 'learning_rate': 0.01280109658499277, 'max_depth': 7, 'num_leaves': 33, 'min_child_samples': 80, 'subsample': 0.7125020710275658, 'colsample_bytree': 0.7200553723535104, 'reg_alpha': 0.027435107766797518, 'reg_lambda': 6.095247024675913}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:09:33,006] Trial 62 finished with value: 0.2848082894979435 and parameters: {'n_estimators': 1000, 'learning_rate': 0.013195144916802374, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 68, 'subsample': 0.7139790168535641, 'colsample_bytree': 0.7314343461338892, 'reg_alpha': 0.030353856083174466, 'reg_lambda': 6.623952646365443}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:10:07,337] Trial 63 finished with value: 0.28398411393214923 and parameters: {'n_estimators': 1009, 'learning_rate': 0.02265984469988625, 'max_depth': 7, 'num_leaves': 23, 'min_child_samples': 59, 'subsample': 0.7320362962120766, 'colsample_bytree': 0.7205599594290508, 'reg_alpha': 0.030168117052650243, 'reg_lambda': 6.86221967422671}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:10:44,438] Trial 64 finished with value: 0.28520975033584606 and parameters: {'n_estimators': 1044, 'learning_rate': 0.01616765848383903, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 72, 'subsample': 0.7171686749512073, 'colsample_bytree': 0.7083984736069631, 'reg_alpha': 0.05435533296884114, 'reg_lambda': 3.6413457686475965}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:11:27,402] Trial 65 finished with value: 0.2687035600197852 and parameters: {'n_estimators': 1043, 'learning_rate': 0.04656288038724893, 'max_depth': 6, 'num_leaves': 36, 'min_child_samples': 74, 'subsample': 0.7184495559971658, 'colsample_bytree': 0.7059357078148515, 'reg_alpha': 0.015484466383849453, 'reg_lambda': 2.2021540668837596}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:12:07,983] Trial 66 finished with value: 0.2841074848338464 and parameters: {'n_estimators': 1072, 'learning_rate': 0.016140502164243592, 'max_depth': 7, 'num_leaves': 27, 'min_child_samples': 69, 'subsample': 0.7280352789546215, 'colsample_bytree': 0.7465407620456707, 'reg_alpha': 0.0374027318261794, 'reg_lambda': 3.44034658241175}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:12:50,774] Trial 67 finished with value: 0.2840509207453839 and parameters: {'n_estimators': 1028, 'learning_rate': 0.019318242330921352, 'max_depth': 7, 'num_leaves': 32, 'min_child_samples': 66, 'subsample': 0.7158200133916293, 'colsample_bytree': 0.7294762796055382, 'reg_alpha': 0.05118230168986845, 'reg_lambda': 4.41784418298113}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:13:25,546] Trial 68 finished with value: 0.28476889327483484 and parameters: {'n_estimators': 1004, 'learning_rate': 0.012486192266608648, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 50, 'subsample': 0.7228933117601084, 'colsample_bytree': 0.7163670278269921, 'reg_alpha': 0.021113212175587908, 'reg_lambda': 7.188862846872504}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:15:02,680] Trial 69 finished with value: 0.2757135894644825 and parameters: {'n_estimators': 1079, 'learning_rate': 0.016423479950576466, 'max_depth': 8, 'num_leaves': 95, 'min_child_samples': 83, 'subsample': 0.7324854251274066, 'colsample_bytree': 0.7002468572030328, 'reg_alpha': 0.02306115284680281, 'reg_lambda': 1.980431611081673}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:15:40,985] Trial 70 finished with value: 0.282857919895525 and parameters: {'n_estimators': 1112, 'learning_rate': 0.02076558073607104, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 93, 'subsample': 0.7502109011881459, 'colsample_bytree': 0.7459114701177563, 'reg_alpha': 0.03200741099429204, 'reg_lambda': 0.03659253128785056}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:16:21,630] Trial 71 finished with value: 0.2851620781769414 and parameters: {'n_estimators': 1058, 'learning_rate': 0.013427203223832077, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 58, 'subsample': 0.7074735649803561, 'colsample_bytree': 0.7097763912977388, 'reg_alpha': 0.054148625471791056, 'reg_lambda': 7.665021309387367}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:17:05,801] Trial 72 finished with value: 0.2849337534018211 and parameters: {'n_estimators': 1064, 'learning_rate': 0.012594366891861612, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 59, 'subsample': 0.7078497779077948, 'colsample_bytree': 0.7087993712793054, 'reg_alpha': 0.05716614707361664, 'reg_lambda': 4.829783416712541}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:17:47,813] Trial 73 finished with value: 0.28494439053804477 and parameters: {'n_estimators': 1061, 'learning_rate': 0.01784734646604141, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 61, 'subsample': 0.7073884322388208, 'colsample_bytree': 0.7082723150904221, 'reg_alpha': 0.10029012197004432, 'reg_lambda': 4.6991755426384065}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:18:39,305] Trial 74 finished with value: 0.2824497676181569 and parameters: {'n_estimators': 1177, 'learning_rate': 0.01903599869978822, 'max_depth': 6, 'num_leaves': 36, 'min_child_samples': 61, 'subsample': 0.7062138541249151, 'colsample_bytree': 0.7087228507581605, 'reg_alpha': 0.09768377372613771, 'reg_lambda': 3.424810950612364}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:19:19,900] Trial 75 finished with value: 0.2847083946595431 and parameters: {'n_estimators': 1070, 'learning_rate': 0.015716715862285756, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 63, 'subsample': 0.720693488089984, 'colsample_bytree': 0.7139102195111487, 'reg_alpha': 0.05396915744034072, 'reg_lambda': 5.05046514219695}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:20:04,850] Trial 76 finished with value: 0.28433232397988745 and parameters: {'n_estimators': 1122, 'learning_rate': 0.01740988675139854, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 55, 'subsample': 0.7058735801771104, 'colsample_bytree': 0.7070429021460053, 'reg_alpha': 0.06267046666137235, 'reg_lambda': 7.808155139405993}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:20:52,964] Trial 77 finished with value: 0.28521022201992247 and parameters: {'n_estimators': 1093, 'learning_rate': 0.010088395320458049, 'max_depth': 6, 'num_leaves': 32, 'min_child_samples': 50, 'subsample': 0.7054636940368021, 'colsample_bytree': 0.7002810221672765, 'reg_alpha': 0.09627064071726839, 'reg_lambda': 2.584958353388713}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:21:34,596] Trial 78 finished with value: 0.28511646184527384 and parameters: {'n_estimators': 1093, 'learning_rate': 0.010294581570244524, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 76, 'subsample': 0.7395766145434733, 'colsample_bytree': 0.7004560179158597, 'reg_alpha': 0.09892693852405776, 'reg_lambda': 2.624564980738292}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:22:23,489] Trial 79 finished with value: 0.2697950464867235 and parameters: {'n_estimators': 1037, 'learning_rate': 0.03929065438253699, 'max_depth': 6, 'num_leaves': 45, 'min_child_samples': 76, 'subsample': 0.7403056617256468, 'colsample_bytree': 0.7069725359722672, 'reg_alpha': 0.08924300572338342, 'reg_lambda': 1.328928141121382}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:23:13,679] Trial 80 finished with value: 0.28023574593213835 and parameters: {'n_estimators': 1095, 'learning_rate': 0.02320596434457845, 'max_depth': 6, 'num_leaves': 40, 'min_child_samples': 57, 'subsample': 0.8032219999179014, 'colsample_bytree': 0.70221500186987, 'reg_alpha': 0.01542934547999386, 'reg_lambda': 2.5793072371669705}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:23:59,922] Trial 81 finished with value: 0.2848818790188621 and parameters: {'n_estimators': 1168, 'learning_rate': 0.010292727410835386, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 69, 'subsample': 0.7071907617984212, 'colsample_bytree': 0.700904993132169, 'reg_alpha': 0.10334872378953773, 'reg_lambda': 4.323302560563932}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:24:36,826] Trial 82 finished with value: 0.2840791017964674 and parameters: {'n_estimators': 1130, 'learning_rate': 0.010002612134168722, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 65, 'subsample': 0.7041141517341775, 'colsample_bytree': 0.7217010365485573, 'reg_alpha': 0.04520775096244681, 'reg_lambda': 3.3193536672113493}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:25:19,860] Trial 83 finished with value: 0.28465196191107134 and parameters: {'n_estimators': 1190, 'learning_rate': 0.014840467608915393, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 110, 'subsample': 0.7268698489796107, 'colsample_bytree': 0.7115164212002004, 'reg_alpha': 0.06197070780129547, 'reg_lambda': 1.860697880457944}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:26:05,782] Trial 84 finished with value: 0.28498158885284325 and parameters: {'n_estimators': 1091, 'learning_rate': 0.012198466641566723, 'max_depth': 6, 'num_leaves': 32, 'min_child_samples': 52, 'subsample': 0.7205623423409881, 'colsample_bytree': 0.7068695438235716, 'reg_alpha': 0.13043388309190193, 'reg_lambda': 5.764468106743204}. Best is trial 45 with value: 0.28530645743654054.\n",
      "[I 2025-06-27 17:26:56,115] Trial 85 finished with value: 0.28532484290404325 and parameters: {'n_estimators': 1095, 'learning_rate': 0.01236717030695383, 'max_depth': 6, 'num_leaves': 36, 'min_child_samples': 54, 'subsample': 0.7337435457563495, 'colsample_bytree': 0.7069406791913154, 'reg_alpha': 0.18729371952576762, 'reg_lambda': 2.4297555195866836}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:27:41,225] Trial 86 finished with value: 0.2830918016297463 and parameters: {'n_estimators': 1026, 'learning_rate': 0.01795631321222546, 'max_depth': 6, 'num_leaves': 35, 'min_child_samples': 54, 'subsample': 0.7346472203844134, 'colsample_bytree': 0.7001906916015193, 'reg_alpha': 0.18018828393412212, 'reg_lambda': 2.2213655103634533}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:28:50,166] Trial 87 finished with value: 0.282228610029415 and parameters: {'n_estimators': 1599, 'learning_rate': 0.014563877302524468, 'max_depth': 6, 'num_leaves': 37, 'min_child_samples': 52, 'subsample': 0.7563503232927771, 'colsample_bytree': 0.7070168224321928, 'reg_alpha': 0.32752754881467994, 'reg_lambda': 2.5581031794159337}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:29:28,779] Trial 88 finished with value: 0.28472488550101793 and parameters: {'n_estimators': 1096, 'learning_rate': 0.011822521994317221, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 71, 'subsample': 0.7422374447093645, 'colsample_bytree': 0.7138408483486303, 'reg_alpha': 0.2021842024017856, 'reg_lambda': 1.7207384472734886}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:30:16,224] Trial 89 finished with value: 0.2821242327641734 and parameters: {'n_estimators': 1217, 'learning_rate': 0.02015209698513487, 'max_depth': 6, 'num_leaves': 32, 'min_child_samples': 63, 'subsample': 0.7180560436272804, 'colsample_bytree': 0.7227872314336381, 'reg_alpha': 0.13562514847448537, 'reg_lambda': 3.0985494126740045}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:30:55,532] Trial 90 finished with value: 0.28267687145259657 and parameters: {'n_estimators': 1104, 'learning_rate': 0.015857789595297156, 'max_depth': 5, 'num_leaves': 31, 'min_child_samples': 56, 'subsample': 0.7233749224405132, 'colsample_bytree': 0.8845002859033803, 'reg_alpha': 0.11939304562448216, 'reg_lambda': 0.1457997652050554}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:31:37,826] Trial 91 finished with value: 0.28507886837417384 and parameters: {'n_estimators': 1065, 'learning_rate': 0.01205830268627718, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 59, 'subsample': 0.7302622960831453, 'colsample_bytree': 0.7082462045610481, 'reg_alpha': 0.10205207617152115, 'reg_lambda': 4.536954226876108}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:32:45,611] Trial 92 finished with value: 0.2835284552250843 and parameters: {'n_estimators': 1482, 'learning_rate': 0.011956136637332838, 'max_depth': 6, 'num_leaves': 40, 'min_child_samples': 62, 'subsample': 0.736596677244874, 'colsample_bytree': 0.7042831895044147, 'reg_alpha': 0.10209821980001259, 'reg_lambda': 3.9423098743916003}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:33:29,181] Trial 93 finished with value: 0.28467566878472417 and parameters: {'n_estimators': 1034, 'learning_rate': 0.014958263001541608, 'max_depth': 6, 'num_leaves': 33, 'min_child_samples': 50, 'subsample': 0.7289637973878627, 'colsample_bytree': 0.7129037275401195, 'reg_alpha': 0.012633021414937107, 'reg_lambda': 5.76520720322284}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:34:15,407] Trial 94 finished with value: 0.2850357708056829 and parameters: {'n_estimators': 1136, 'learning_rate': 0.011881334370020908, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 79, 'subsample': 0.7205000013641036, 'colsample_bytree': 0.7182272167830817, 'reg_alpha': 0.14004484048663363, 'reg_lambda': 8.121060326239201}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:34:56,952] Trial 95 finished with value: 0.28468168836594876 and parameters: {'n_estimators': 1139, 'learning_rate': 0.01201678189701208, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 77, 'subsample': 0.7304032349645989, 'colsample_bytree': 0.7186410993622353, 'reg_alpha': 0.20554566260019314, 'reg_lambda': 7.767210502823158}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:35:56,947] Trial 96 finished with value: 0.2843054075773103 and parameters: {'n_estimators': 1088, 'learning_rate': 0.011623126731931721, 'max_depth': 6, 'num_leaves': 87, 'min_child_samples': 88, 'subsample': 0.7505242891492887, 'colsample_bytree': 0.7259623868080692, 'reg_alpha': 0.010226710064409247, 'reg_lambda': 3.5856020294405617}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:36:43,571] Trial 97 finished with value: 0.28466820392899084 and parameters: {'n_estimators': 1158, 'learning_rate': 0.014022903582792961, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 72, 'subsample': 0.7184471036897911, 'colsample_bytree': 0.7313019884310977, 'reg_alpha': 0.1701980561018879, 'reg_lambda': 0.5795416091167364}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:37:58,009] Trial 98 finished with value: 0.2801280488255505 and parameters: {'n_estimators': 1802, 'learning_rate': 0.016451662733693532, 'max_depth': 6, 'num_leaves': 34, 'min_child_samples': 80, 'subsample': 0.7233930025896278, 'colsample_bytree': 0.7044423680262392, 'reg_alpha': 0.36091193838033636, 'reg_lambda': 2.3916410825952608}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:38:37,738] Trial 99 finished with value: 0.2847097270160238 and parameters: {'n_estimators': 1129, 'learning_rate': 0.011475836850425573, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 66, 'subsample': 0.7359494080669701, 'colsample_bytree': 0.7158059304062327, 'reg_alpha': 0.13687970010847486, 'reg_lambda': 8.468558176653344}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:39:23,620] Trial 100 finished with value: 0.2844802468642673 and parameters: {'n_estimators': 1253, 'learning_rate': 0.013918539894874331, 'max_depth': 5, 'num_leaves': 29, 'min_child_samples': 57, 'subsample': 0.7769690720281995, 'colsample_bytree': 0.7100779560680799, 'reg_alpha': 0.06743618079709397, 'reg_lambda': 1.415228444642919}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:40:02,294] Trial 101 finished with value: 0.28499479697895247 and parameters: {'n_estimators': 1049, 'learning_rate': 0.018079253555938538, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 61, 'subsample': 0.7131423440822888, 'colsample_bytree': 0.7000403366761984, 'reg_alpha': 0.08247887949499443, 'reg_lambda': 4.374056720012877}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:40:37,331] Trial 102 finished with value: 0.26925257170683936 and parameters: {'n_estimators': 1021, 'learning_rate': 0.065924443782185, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 53, 'subsample': 0.7136117928035652, 'colsample_bytree': 0.7051892213101072, 'reg_alpha': 0.08280818382620105, 'reg_lambda': 4.241779703978905}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:41:18,495] Trial 103 finished with value: 0.2845518602409452 and parameters: {'n_estimators': 1104, 'learning_rate': 0.015516833918819428, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 88, 'subsample': 0.7112456826841814, 'colsample_bytree': 0.7001120780094843, 'reg_alpha': 0.1409955588467343, 'reg_lambda': 5.884639880472762}. Best is trial 85 with value: 0.28532484290404325.\n",
      "[I 2025-06-27 17:42:03,237] Trial 104 finished with value: 0.2853265535093311 and parameters: {'n_estimators': 1055, 'learning_rate': 0.01014495550288553, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 58, 'subsample': 0.7202634383534275, 'colsample_bytree': 0.7194652057849664, 'reg_alpha': 0.10894288383090987, 'reg_lambda': 2.8251074813554533}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:43:00,705] Trial 105 finished with value: 0.28425578309716537 and parameters: {'n_estimators': 1041, 'learning_rate': 0.01018813790942663, 'max_depth': 6, 'num_leaves': 57, 'min_child_samples': 71, 'subsample': 0.7266270586281692, 'colsample_bytree': 0.7183617681220046, 'reg_alpha': 0.11103793234669011, 'reg_lambda': 0.9998477531305866}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:43:37,596] Trial 106 finished with value: 0.2851574137036649 and parameters: {'n_estimators': 1064, 'learning_rate': 0.013903133216347728, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 64, 'subsample': 0.7168635272100954, 'colsample_bytree': 0.7125958846388117, 'reg_alpha': 0.046815828378495045, 'reg_lambda': 2.7930641026259138}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:44:14,986] Trial 107 finished with value: 0.28462556509969394 and parameters: {'n_estimators': 1077, 'learning_rate': 0.013664000906477647, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 127, 'subsample': 0.7325084026929696, 'colsample_bytree': 0.7642008480659513, 'reg_alpha': 0.21998094966630224, 'reg_lambda': 2.8382849048925998}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:44:50,304] Trial 108 finished with value: 0.2842934564316293 and parameters: {'n_estimators': 1018, 'learning_rate': 0.0113563706802046, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 133, 'subsample': 0.743917985319409, 'colsample_bytree': 0.7920212262765767, 'reg_alpha': 0.04834715885185265, 'reg_lambda': 2.04693489861756}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:45:32,849] Trial 109 finished with value: 0.2848118910218357 and parameters: {'n_estimators': 1115, 'learning_rate': 0.014388635412811771, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 65, 'subsample': 0.7234511149018205, 'colsample_bytree': 0.7227536828272619, 'reg_alpha': 0.07322799349917854, 'reg_lambda': 3.189649856121428}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:46:25,739] Trial 110 finished with value: 0.2821733976778495 and parameters: {'n_estimators': 1387, 'learning_rate': 0.01703191832306722, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 58, 'subsample': 0.7182776375500116, 'colsample_bytree': 0.8247420053219032, 'reg_alpha': 0.03546864629090911, 'reg_lambda': 2.541624233444445}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:47:05,065] Trial 111 finished with value: 0.2843125340047541 and parameters: {'n_estimators': 1055, 'learning_rate': 0.013007919589190812, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 68, 'subsample': 0.7159646665018673, 'colsample_bytree': 0.7125176692834173, 'reg_alpha': 0.09167165278487355, 'reg_lambda': 4.18476519413921}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:47:42,684] Trial 112 finished with value: 0.2847338126876396 and parameters: {'n_estimators': 1055, 'learning_rate': 0.01859087297493196, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 61, 'subsample': 0.7116856141231309, 'colsample_bytree': 0.7116053324098889, 'reg_alpha': 0.04357318198359478, 'reg_lambda': 4.9067860057257136}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:48:20,745] Trial 113 finished with value: 0.27116878890248963 and parameters: {'n_estimators': 1072, 'learning_rate': 0.05210294434677683, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 77, 'subsample': 0.7041691819352394, 'colsample_bytree': 0.7164868172918657, 'reg_alpha': 0.08086233347063938, 'reg_lambda': 3.591222769600466}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:49:05,775] Trial 114 finished with value: 0.2849653483123108 and parameters: {'n_estimators': 1033, 'learning_rate': 0.01105138514011742, 'max_depth': 6, 'num_leaves': 33, 'min_child_samples': 56, 'subsample': 0.7399035852476765, 'colsample_bytree': 0.7038045693314682, 'reg_alpha': 0.06400987070436759, 'reg_lambda': 1.707341897360548}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:49:43,433] Trial 115 finished with value: 0.2849775817272518 and parameters: {'n_estimators': 1143, 'learning_rate': 0.015439088937227036, 'max_depth': 6, 'num_leaves': 21, 'min_child_samples': 112, 'subsample': 0.7269515713469219, 'colsample_bytree': 0.7089610229974114, 'reg_alpha': 0.15591544104164054, 'reg_lambda': 6.966482623818425}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:50:28,759] Trial 116 finished with value: 0.2822942318543709 and parameters: {'n_estimators': 1090, 'learning_rate': 0.02096453065714514, 'max_depth': 6, 'num_leaves': 35, 'min_child_samples': 65, 'subsample': 0.732014008806872, 'colsample_bytree': 0.7204439090197324, 'reg_alpha': 0.11180656474560381, 'reg_lambda': 8.534222568182136}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:51:05,658] Trial 117 finished with value: 0.2848293871970217 and parameters: {'n_estimators': 1000, 'learning_rate': 0.012735096613950768, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 59, 'subsample': 0.7115509349998217, 'colsample_bytree': 0.7251444531203064, 'reg_alpha': 0.013387377140694637, 'reg_lambda': 2.7570213438951425}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:51:46,563] Trial 118 finished with value: 0.2726709721454252 and parameters: {'n_estimators': 1118, 'learning_rate': 0.043558973765136975, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 156, 'subsample': 0.716956835442663, 'colsample_bytree': 0.7145391138433423, 'reg_alpha': 0.29364128567986514, 'reg_lambda': 2.2582341627678635}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:52:51,820] Trial 119 finished with value: 0.28399951070992996 and parameters: {'n_estimators': 1187, 'learning_rate': 0.010073213650754607, 'max_depth': 6, 'num_leaves': 76, 'min_child_samples': 74, 'subsample': 0.7217317539672139, 'colsample_bytree': 0.7000375623586392, 'reg_alpha': 0.07336966286843583, 'reg_lambda': 0.07958308555861116}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:53:28,010] Trial 120 finished with value: 0.28485502819955083 and parameters: {'n_estimators': 1060, 'learning_rate': 0.016209916681757836, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 54, 'subsample': 0.7107892075486678, 'colsample_bytree': 0.7048254374656878, 'reg_alpha': 0.09128347470371581, 'reg_lambda': 6.251938966722462}. Best is trial 104 with value: 0.2853265535093311.\n",
      "[I 2025-06-27 17:54:12,662] Trial 121 finished with value: 0.2853643122374576 and parameters: {'n_estimators': 1090, 'learning_rate': 0.012334712019928034, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 52, 'subsample': 0.7200365507870846, 'colsample_bytree': 0.7085934944017899, 'reg_alpha': 0.12953139197938843, 'reg_lambda': 5.336672402025752}. Best is trial 121 with value: 0.2853643122374576.\n",
      "[I 2025-06-27 17:54:52,889] Trial 122 finished with value: 0.2853958763193618 and parameters: {'n_estimators': 1038, 'learning_rate': 0.013281377932256606, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 50, 'subsample': 0.7290404930495044, 'colsample_bytree': 0.7101352369500463, 'reg_alpha': 0.18001601412071802, 'reg_lambda': 4.7290215803443365}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:55:34,637] Trial 123 finished with value: 0.2852995137609705 and parameters: {'n_estimators': 1081, 'learning_rate': 0.013528108814185945, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 53, 'subsample': 0.8163425367116524, 'colsample_bytree': 0.709510831740695, 'reg_alpha': 0.2348882235168562, 'reg_lambda': 5.575993340074317}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:56:14,201] Trial 124 finished with value: 0.2797996532760649 and parameters: {'n_estimators': 1022, 'learning_rate': 0.033169951689957906, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 50, 'subsample': 0.823795952602895, 'colsample_bytree': 0.7104638047593053, 'reg_alpha': 0.45824103414802264, 'reg_lambda': 3.2099273889695166}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:57:01,798] Trial 125 finished with value: 0.2841676007639583 and parameters: {'n_estimators': 1082, 'learning_rate': 0.013868462799115586, 'max_depth': 6, 'num_leaves': 36, 'min_child_samples': 55, 'subsample': 0.7373046422953593, 'colsample_bytree': 0.707360102674189, 'reg_alpha': 0.18108383061605637, 'reg_lambda': 5.2607179276853175}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:58:03,891] Trial 126 finished with value: 0.2835836565457301 and parameters: {'n_estimators': 1723, 'learning_rate': 0.012989175014963349, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 52, 'subsample': 0.8319777290805017, 'colsample_bytree': 0.7136738697111401, 'reg_alpha': 0.6844241167101289, 'reg_lambda': 0.31215900632389504}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:58:43,248] Trial 127 finished with value: 0.2839242006243681 and parameters: {'n_estimators': 1105, 'learning_rate': 0.010102717847914605, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 58, 'subsample': 0.8197416611547595, 'colsample_bytree': 0.8483381006734563, 'reg_alpha': 0.23630084319941724, 'reg_lambda': 3.7287387389022433}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 17:59:28,286] Trial 128 finished with value: 0.285020316885491 and parameters: {'n_estimators': 1066, 'learning_rate': 0.011641229450214126, 'max_depth': 6, 'num_leaves': 33, 'min_child_samples': 50, 'subsample': 0.7474788897078565, 'colsample_bytree': 0.7299999422114939, 'reg_alpha': 0.01623869723373636, 'reg_lambda': 4.556618214968914}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:00:05,347] Trial 129 finished with value: 0.2851369899895575 and parameters: {'n_estimators': 1162, 'learning_rate': 0.014632217705439215, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 64, 'subsample': 0.8131877728559116, 'colsample_bytree': 0.7048561955359128, 'reg_alpha': 0.25950239827010246, 'reg_lambda': 6.889110678247292}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:00:43,186] Trial 130 finished with value: 0.2848264520077058 and parameters: {'n_estimators': 1160, 'learning_rate': 0.014783954815607165, 'max_depth': 6, 'num_leaves': 21, 'min_child_samples': 63, 'subsample': 0.8144697906341257, 'colsample_bytree': 0.7048662573259654, 'reg_alpha': 0.26696545651516634, 'reg_lambda': 7.025897033835287}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:01:16,932] Trial 131 finished with value: 0.28527481434527135 and parameters: {'n_estimators': 1038, 'learning_rate': 0.01330730246097885, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 55, 'subsample': 0.8033665394775727, 'colsample_bytree': 0.7094698499810701, 'reg_alpha': 0.19774129282953892, 'reg_lambda': 5.531060644110433}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:01:51,369] Trial 132 finished with value: 0.28441648558780325 and parameters: {'n_estimators': 1033, 'learning_rate': 0.016484422023942014, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 55, 'subsample': 0.8051011441261985, 'colsample_bytree': 0.7104835392679345, 'reg_alpha': 0.1910654261265144, 'reg_lambda': 5.364365797399874}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:02:29,533] Trial 133 finished with value: 0.28493853466406927 and parameters: {'n_estimators': 1091, 'learning_rate': 0.01419097598888176, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 68, 'subsample': 0.7993797006600267, 'colsample_bytree': 0.7165712802087439, 'reg_alpha': 0.35560723023020774, 'reg_lambda': 6.55173959069548}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:03:01,974] Trial 134 finished with value: 0.28459638984617175 and parameters: {'n_estimators': 1001, 'learning_rate': 0.013157738827198624, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 54, 'subsample': 0.800753467326315, 'colsample_bytree': 0.7217021622610704, 'reg_alpha': 0.2417352534739952, 'reg_lambda': 7.410321325474892}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:03:37,045] Trial 135 finished with value: 0.28532656150249125 and parameters: {'n_estimators': 1118, 'learning_rate': 0.017449857367093664, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 63, 'subsample': 0.7002377922258626, 'colsample_bytree': 0.703646102109027, 'reg_alpha': 0.29853661486213473, 'reg_lambda': 5.583453483725868}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:04:09,771] Trial 136 finished with value: 0.2847997827958243 and parameters: {'n_estimators': 1044, 'learning_rate': 0.019188556519374685, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 63, 'subsample': 0.7936435540964761, 'colsample_bytree': 0.7040444526069385, 'reg_alpha': 0.2799495754615476, 'reg_lambda': 5.5848525703078415}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:04:44,479] Trial 137 finished with value: 0.285017695043021 and parameters: {'n_estimators': 1120, 'learning_rate': 0.017321623488138687, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 58, 'subsample': 0.7004186828898922, 'colsample_bytree': 0.7130382720128897, 'reg_alpha': 0.4240123879184115, 'reg_lambda': 6.086416955972461}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:05:23,482] Trial 138 finished with value: 0.2853792505596655 and parameters: {'n_estimators': 1162, 'learning_rate': 0.014569728402891783, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 50, 'subsample': 0.8071434045501509, 'colsample_bytree': 0.7081741870196355, 'reg_alpha': 0.33280650772237036, 'reg_lambda': 8.508482575611566}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:06:00,330] Trial 139 finished with value: 0.2852588177826584 and parameters: {'n_estimators': 1077, 'learning_rate': 0.01680156648150409, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 53, 'subsample': 0.8132048113978195, 'colsample_bytree': 0.709842472248311, 'reg_alpha': 0.5833719650097566, 'reg_lambda': 8.802360890379497}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:06:36,721] Trial 140 finished with value: 0.28488937732932706 and parameters: {'n_estimators': 1128, 'learning_rate': 0.01986416588352489, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 50, 'subsample': 0.8093513396307673, 'colsample_bytree': 0.7195034995211816, 'reg_alpha': 0.32742388775549064, 'reg_lambda': 7.74995324590613}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:07:35,159] Trial 141 finished with value: 0.2830444026429162 and parameters: {'n_estimators': 1075, 'learning_rate': 0.015740975619977754, 'max_depth': 6, 'num_leaves': 100, 'min_child_samples': 52, 'subsample': 0.8140676464292806, 'colsample_bytree': 0.7093676821912288, 'reg_alpha': 0.5320002885670405, 'reg_lambda': 8.649845906331581}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:08:13,502] Trial 142 finished with value: 0.2847121952872839 and parameters: {'n_estimators': 1105, 'learning_rate': 0.017403167394788074, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 56, 'subsample': 0.8087360664928875, 'colsample_bytree': 0.7138710275866078, 'reg_alpha': 0.16464053759974778, 'reg_lambda': 5.117756831048953}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:08:50,542] Trial 143 finished with value: 0.2849917832446841 and parameters: {'n_estimators': 1047, 'learning_rate': 0.013134162391197555, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 60, 'subsample': 0.8267847424538496, 'colsample_bytree': 0.7088023819332647, 'reg_alpha': 0.920507002770548, 'reg_lambda': 9.209920887679269}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:09:29,900] Trial 144 finished with value: 0.28498060893517374 and parameters: {'n_estimators': 1017, 'learning_rate': 0.01159354747088719, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 53, 'subsample': 0.7980985014981219, 'colsample_bytree': 0.7163210448431749, 'reg_alpha': 0.726372522537605, 'reg_lambda': 9.946357469792408}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:10:07,192] Trial 145 finished with value: 0.2853043627976609 and parameters: {'n_estimators': 1078, 'learning_rate': 0.015132222286054483, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 56, 'subsample': 0.8085694457695166, 'colsample_bytree': 0.7254947114522438, 'reg_alpha': 0.5427479358856311, 'reg_lambda': 3.9806635364892085}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:10:47,989] Trial 146 finished with value: 0.28467415370944893 and parameters: {'n_estimators': 1084, 'learning_rate': 0.015510988285876128, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 55, 'subsample': 0.8189076393202177, 'colsample_bytree': 0.7246490071963124, 'reg_alpha': 0.4912032265602897, 'reg_lambda': 6.282256475439748}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:11:46,141] Trial 147 finished with value: 0.2795317138687883 and parameters: {'n_estimators': 1909, 'learning_rate': 0.026450205386006734, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 52, 'subsample': 0.8051447525612802, 'colsample_bytree': 0.704272725013954, 'reg_alpha': 0.41333008161211776, 'reg_lambda': 3.8802996898750366}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:12:30,840] Trial 148 finished with value: 0.28236785489250343 and parameters: {'n_estimators': 1147, 'learning_rate': 0.02199559424551218, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 58, 'subsample': 0.795884162136454, 'colsample_bytree': 0.7276851414108974, 'reg_alpha': 0.5540644123345915, 'reg_lambda': 4.870666229739725}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:13:08,032] Trial 149 finished with value: 0.284550547781032 and parameters: {'n_estimators': 1035, 'learning_rate': 0.018136417148602296, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 50, 'subsample': 0.8164306097294293, 'colsample_bytree': 0.7200828711649097, 'reg_alpha': 0.2132477250055822, 'reg_lambda': 5.415039978483291}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:13:46,413] Trial 150 finished with value: 0.270814125294225 and parameters: {'n_estimators': 1108, 'learning_rate': 0.06090137600350736, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 56, 'subsample': 0.8097652388263088, 'colsample_bytree': 0.7072631066077155, 'reg_alpha': 0.5788627165649293, 'reg_lambda': 7.844171449305388}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:14:24,262] Trial 151 finished with value: 0.28516902014866063 and parameters: {'n_estimators': 1060, 'learning_rate': 0.014372358849440206, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 61, 'subsample': 0.8231980924751593, 'colsample_bytree': 0.7116424645904941, 'reg_alpha': 0.3017957065809637, 'reg_lambda': 3.163688720736494}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:15:24,658] Trial 152 finished with value: 0.283512846586668 and parameters: {'n_estimators': 1054, 'learning_rate': 0.012284596375131292, 'max_depth': 6, 'num_leaves': 79, 'min_child_samples': 60, 'subsample': 0.8295504811742679, 'colsample_bytree': 0.7112898372651937, 'reg_alpha': 0.38122391191783656, 'reg_lambda': 4.028665851987702}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:16:01,255] Trial 153 finished with value: 0.2847868515340691 and parameters: {'n_estimators': 1082, 'learning_rate': 0.016501566761863377, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 53, 'subsample': 0.8091144012133865, 'colsample_bytree': 0.7027930641133867, 'reg_alpha': 0.1962079243726196, 'reg_lambda': 3.1738979100829687}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:16:38,805] Trial 154 finished with value: 0.28483775054941557 and parameters: {'n_estimators': 1024, 'learning_rate': 0.014618998825653111, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 61, 'subsample': 0.8174012339030581, 'colsample_bytree': 0.7164000560051059, 'reg_alpha': 0.2952159845846431, 'reg_lambda': 6.2378528659743635}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:17:23,668] Trial 155 finished with value: 0.2852769826654534 and parameters: {'n_estimators': 1126, 'learning_rate': 0.011211725222755337, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 56, 'subsample': 0.8216649130235398, 'colsample_bytree': 0.70712199227821, 'reg_alpha': 0.6413921119332701, 'reg_lambda': 4.6191329065312665}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:18:11,539] Trial 156 finished with value: 0.28497044671028177 and parameters: {'n_estimators': 1175, 'learning_rate': 0.01129476644966989, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 55, 'subsample': 0.8228324565879388, 'colsample_bytree': 0.706604186780918, 'reg_alpha': 1.2793727615735895, 'reg_lambda': 4.482768237162464}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:19:02,523] Trial 157 finished with value: 0.2848922443822455 and parameters: {'n_estimators': 1205, 'learning_rate': 0.010215446455646874, 'max_depth': 6, 'num_leaves': 32, 'min_child_samples': 101, 'subsample': 0.8374785141352524, 'colsample_bytree': 0.70022116832165, 'reg_alpha': 0.652182555003151, 'reg_lambda': 3.501772390500578}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:19:46,295] Trial 158 finished with value: 0.2842697495402632 and parameters: {'n_estimators': 1127, 'learning_rate': 0.012595330113047613, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 50, 'subsample': 0.823561720562811, 'colsample_bytree': 0.8000682138997771, 'reg_alpha': 0.8484322332028101, 'reg_lambda': 3.951330662404061}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:20:32,474] Trial 159 finished with value: 0.2844679805427307 and parameters: {'n_estimators': 1095, 'learning_rate': 0.015465599086148227, 'max_depth': 6, 'num_leaves': 34, 'min_child_samples': 57, 'subsample': 0.8431793607847841, 'colsample_bytree': 0.713939660505789, 'reg_alpha': 0.3426714711471999, 'reg_lambda': 4.62339820968926}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:21:15,206] Trial 160 finished with value: 0.2849351881453955 and parameters: {'n_estimators': 1109, 'learning_rate': 0.013634509032576469, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 121, 'subsample': 0.8061878068501949, 'colsample_bytree': 0.7217482329462892, 'reg_alpha': 0.22366333033608127, 'reg_lambda': 3.041203310240015}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:21:54,976] Trial 161 finished with value: 0.28454783520344246 and parameters: {'n_estimators': 1069, 'learning_rate': 0.011414832609183544, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 54, 'subsample': 0.8127785595432062, 'colsample_bytree': 0.7089738307316935, 'reg_alpha': 0.15348356526296766, 'reg_lambda': 5.44332602775794}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:22:33,748] Trial 162 finished with value: 0.28507460912053795 and parameters: {'n_estimators': 1052, 'learning_rate': 0.012977346086776965, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 61, 'subsample': 0.7054232723160212, 'colsample_bytree': 0.7112391782800089, 'reg_alpha': 0.4636895368253275, 'reg_lambda': 7.337904369171893}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:23:18,370] Trial 163 finished with value: 0.28523798487210406 and parameters: {'n_estimators': 1132, 'learning_rate': 0.014022451290126817, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 58, 'subsample': 0.8274452609641381, 'colsample_bytree': 0.703701965823568, 'reg_alpha': 1.0317054430502597, 'reg_lambda': 8.703640424030317}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:23:57,760] Trial 164 finished with value: 0.28514087927848275 and parameters: {'n_estimators': 1148, 'learning_rate': 0.014927239850596642, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 53, 'subsample': 0.8349909865247018, 'colsample_bytree': 0.70515162282, 'reg_alpha': 0.9388048722778458, 'reg_lambda': 9.001360713777363}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:24:37,854] Trial 165 finished with value: 0.28058402572730645 and parameters: {'n_estimators': 1127, 'learning_rate': 0.030524446263883488, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 67, 'subsample': 0.828009152091215, 'colsample_bytree': 0.7170102977333671, 'reg_alpha': 1.1569293146806383, 'reg_lambda': 6.2495952558644055}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:25:37,601] Trial 166 finished with value: 0.2846236765857921 and parameters: {'n_estimators': 1095, 'learning_rate': 0.010126310462732843, 'max_depth': 6, 'num_leaves': 71, 'min_child_samples': 191, 'subsample': 0.8200172062510364, 'colsample_bytree': 0.7031786770372117, 'reg_alpha': 0.7589311075594184, 'reg_lambda': 4.9957997855103216}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:26:22,701] Trial 167 finished with value: 0.28416842963813116 and parameters: {'n_estimators': 1141, 'learning_rate': 0.016745521005642694, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 57, 'subsample': 0.8254322648236504, 'colsample_bytree': 0.7000244693680524, 'reg_alpha': 1.4295367864428445, 'reg_lambda': 3.614710065739255}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:27:04,652] Trial 168 finished with value: 0.28470946458447227 and parameters: {'n_estimators': 1075, 'learning_rate': 0.011702769061488741, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 114, 'subsample': 0.8027131724231139, 'colsample_bytree': 0.7076681548109415, 'reg_alpha': 0.010081083817381811, 'reg_lambda': 4.192822568746246}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:27:51,501] Trial 169 finished with value: 0.2843229756903116 and parameters: {'n_estimators': 1175, 'learning_rate': 0.014211329869612762, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 61, 'subsample': 0.8156000212514553, 'colsample_bytree': 0.711541774636191, 'reg_alpha': 0.12235096492540891, 'reg_lambda': 0.02958090524957331}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:28:25,756] Trial 170 finished with value: 0.2846151484277204 and parameters: {'n_estimators': 1038, 'learning_rate': 0.018443591193927446, 'max_depth': 6, 'num_leaves': 21, 'min_child_samples': 148, 'subsample': 0.7882262686715223, 'colsample_bytree': 0.7061935691480075, 'reg_alpha': 0.3035264907911394, 'reg_lambda': 8.40147792234742}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:29:05,562] Trial 171 finished with value: 0.2844410914148915 and parameters: {'n_estimators': 1068, 'learning_rate': 0.013276822943778922, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 57, 'subsample': 0.8316764836645925, 'colsample_bytree': 0.8617680823933302, 'reg_alpha': 0.9918597323220976, 'reg_lambda': 6.840082788463459}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:29:45,048] Trial 172 finished with value: 0.28519537412740775 and parameters: {'n_estimators': 1108, 'learning_rate': 0.012404975062046507, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 50, 'subsample': 0.7039730251497599, 'colsample_bytree': 0.7103163537906951, 'reg_alpha': 0.17375953057509322, 'reg_lambda': 5.80545517787562}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:30:22,950] Trial 173 finished with value: 0.2847151118487782 and parameters: {'n_estimators': 1116, 'learning_rate': 0.011398624928525003, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 50, 'subsample': 0.7038833858090052, 'colsample_bytree': 0.7147162741796913, 'reg_alpha': 0.6244936856883618, 'reg_lambda': 5.874378987662488}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:31:01,680] Trial 174 finished with value: 0.28466134549687777 and parameters: {'n_estimators': 1100, 'learning_rate': 0.015676432340005483, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 53, 'subsample': 0.8118943205706282, 'colsample_bytree': 0.7036051637786203, 'reg_alpha': 0.18575711509964365, 'reg_lambda': 4.811448869789874}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:31:45,076] Trial 175 finished with value: 0.28487410266977115 and parameters: {'n_estimators': 1088, 'learning_rate': 0.012334147663345493, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 50, 'subsample': 0.8215273345239287, 'colsample_bytree': 0.7760673949201603, 'reg_alpha': 0.16889196779961113, 'reg_lambda': 9.991214584836857}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:32:29,109] Trial 176 finished with value: 0.2851879279953011 and parameters: {'n_estimators': 1156, 'learning_rate': 0.01010352843642294, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 187, 'subsample': 0.7236215491484307, 'colsample_bytree': 0.7100701277037793, 'reg_alpha': 0.24844702718433284, 'reg_lambda': 6.805175180633017}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:33:16,897] Trial 177 finished with value: 0.2850721898865747 and parameters: {'n_estimators': 1193, 'learning_rate': 0.01010963347898659, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 198, 'subsample': 0.7261628831611749, 'colsample_bytree': 0.7081299713680912, 'reg_alpha': 0.2481516184512953, 'reg_lambda': 6.910382989269507}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:34:06,761] Trial 178 finished with value: 0.28489321830398695 and parameters: {'n_estimators': 1154, 'learning_rate': 0.011457420794942537, 'max_depth': 6, 'num_leaves': 32, 'min_child_samples': 196, 'subsample': 0.722567281450415, 'colsample_bytree': 0.715821788637723, 'reg_alpha': 0.12703716373331111, 'reg_lambda': 5.534328766370237}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:34:52,373] Trial 179 finished with value: 0.2851142726485291 and parameters: {'n_estimators': 1222, 'learning_rate': 0.012627731897052887, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 188, 'subsample': 0.7188929679668066, 'colsample_bytree': 0.7201912697224105, 'reg_alpha': 0.1446464938730616, 'reg_lambda': 7.215552738013922}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:35:30,578] Trial 180 finished with value: 0.2849069641643749 and parameters: {'n_estimators': 1133, 'learning_rate': 0.01116587302352046, 'max_depth': 6, 'num_leaves': 20, 'min_child_samples': 195, 'subsample': 0.7088961555886336, 'colsample_bytree': 0.7036100589190218, 'reg_alpha': 0.20540950751831527, 'reg_lambda': 8.421744994550911}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:36:09,338] Trial 181 finished with value: 0.2850555992960389 and parameters: {'n_estimators': 1109, 'learning_rate': 0.013983509063508797, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 54, 'subsample': 0.7139786430058023, 'colsample_bytree': 0.7106222509766026, 'reg_alpha': 0.23389181762183026, 'reg_lambda': 5.887069074772258}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:36:47,839] Trial 182 finished with value: 0.28400862032595386 and parameters: {'n_estimators': 1015, 'learning_rate': 0.01002148711199096, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 58, 'subsample': 0.728499997428196, 'colsample_bytree': 0.711818096870061, 'reg_alpha': 0.31616661570341736, 'reg_lambda': 3.406203792974106}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:37:26,275] Trial 183 finished with value: 0.25701734807720367 and parameters: {'n_estimators': 1048, 'learning_rate': 0.07224413256993742, 'max_depth': 6, 'num_leaves': 30, 'min_child_samples': 192, 'subsample': 0.8070740122687846, 'colsample_bytree': 0.7071514140332292, 'reg_alpha': 0.4014424086994396, 'reg_lambda': 0.01919642000975466}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:38:24,967] Trial 184 finished with value: 0.2834785857324684 and parameters: {'n_estimators': 1081, 'learning_rate': 0.014307453486484002, 'max_depth': 6, 'num_leaves': 65, 'min_child_samples': 55, 'subsample': 0.7244595458744427, 'colsample_bytree': 0.7142793179520709, 'reg_alpha': 0.2576265412475299, 'reg_lambda': 4.578213153494206}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:39:04,779] Trial 185 finished with value: 0.28492566464662195 and parameters: {'n_estimators': 1167, 'learning_rate': 0.016295729936129846, 'max_depth': 6, 'num_leaves': 23, 'min_child_samples': 183, 'subsample': 0.702553368866817, 'colsample_bytree': 0.700086530024772, 'reg_alpha': 0.17429927966555364, 'reg_lambda': 2.376746813849939}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:39:48,478] Trial 186 finished with value: 0.28456154566930947 and parameters: {'n_estimators': 1120, 'learning_rate': 0.012704147870565674, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 63, 'subsample': 0.8167737512436929, 'colsample_bytree': 0.7070289241055056, 'reg_alpha': 0.017791853731198223, 'reg_lambda': 6.333046900220611}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:41:04,940] Trial 187 finished with value: 0.27992884552028074 and parameters: {'n_estimators': 1538, 'learning_rate': 0.01480744332110696, 'max_depth': 6, 'num_leaves': 53, 'min_child_samples': 169, 'subsample': 0.7158855266151877, 'colsample_bytree': 0.7174357679292388, 'reg_alpha': 0.21071154543680368, 'reg_lambda': 4.04291540564834}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:41:43,768] Trial 188 finished with value: 0.2852098230218743 and parameters: {'n_estimators': 1058, 'learning_rate': 0.013300562661425885, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 52, 'subsample': 0.8002082152425017, 'colsample_bytree': 0.7102332840757093, 'reg_alpha': 0.8117268107129524, 'reg_lambda': 5.474319118724734}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:42:32,911] Trial 189 finished with value: 0.2845377180797296 and parameters: {'n_estimators': 1147, 'learning_rate': 0.012411938914262313, 'max_depth': 6, 'num_leaves': 33, 'min_child_samples': 52, 'subsample': 0.7982670940560161, 'colsample_bytree': 0.7042676753106722, 'reg_alpha': 0.813706738071532, 'reg_lambda': 5.525718852336156}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:43:09,452] Trial 190 finished with value: 0.2803519501558983 and parameters: {'n_estimators': 1032, 'learning_rate': 0.037064875927251925, 'max_depth': 6, 'num_leaves': 26, 'min_child_samples': 143, 'subsample': 0.7928132217688681, 'colsample_bytree': 0.7096016862642652, 'reg_alpha': 1.047027915417159, 'reg_lambda': 7.697988446095714}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:43:48,396] Trial 191 finished with value: 0.28517179696846534 and parameters: {'n_estimators': 1060, 'learning_rate': 0.01357961480057287, 'max_depth': 6, 'num_leaves': 24, 'min_child_samples': 50, 'subsample': 0.8021607473019493, 'colsample_bytree': 0.7119096239061841, 'reg_alpha': 0.5108701494047796, 'reg_lambda': 5.062741290420372}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:44:25,353] Trial 192 finished with value: 0.2848207603138874 and parameters: {'n_estimators': 1077, 'learning_rate': 0.013206947956004678, 'max_depth': 6, 'num_leaves': 22, 'min_child_samples': 50, 'subsample': 0.8016450093005117, 'colsample_bytree': 0.7137924232028569, 'reg_alpha': 0.6046368025576565, 'reg_lambda': 5.077161427047647}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:45:09,491] Trial 193 finished with value: 0.28518512841965477 and parameters: {'n_estimators': 1095, 'learning_rate': 0.011310509007321378, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 56, 'subsample': 0.8044471414387856, 'colsample_bytree': 0.7036344036358539, 'reg_alpha': 0.7657241566096338, 'reg_lambda': 6.430287361873628}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:45:54,473] Trial 194 finished with value: 0.2849496166616425 and parameters: {'n_estimators': 1100, 'learning_rate': 0.01125875920069247, 'max_depth': 6, 'num_leaves': 29, 'min_child_samples': 56, 'subsample': 0.8075049296569754, 'colsample_bytree': 0.7027591099173965, 'reg_alpha': 0.15464908268354433, 'reg_lambda': 6.684039726109566}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:46:39,809] Trial 195 finished with value: 0.28466141917398824 and parameters: {'n_estimators': 1129, 'learning_rate': 0.010070585654658426, 'max_depth': 6, 'num_leaves': 27, 'min_child_samples': 53, 'subsample': 0.810860871252398, 'colsample_bytree': 0.707011092757551, 'reg_alpha': 0.750295547941105, 'reg_lambda': 8.208709448403756}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:47:25,927] Trial 196 finished with value: 0.28481830898379185 and parameters: {'n_estimators': 1091, 'learning_rate': 0.01167776444395266, 'max_depth': 6, 'num_leaves': 31, 'min_child_samples': 59, 'subsample': 0.7863320014180758, 'colsample_bytree': 0.7035059274196119, 'reg_alpha': 0.863487715498182, 'reg_lambda': 5.786626085581166}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:48:08,542] Trial 197 finished with value: 0.28433136095409883 and parameters: {'n_estimators': 1114, 'learning_rate': 0.015541340230773398, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 55, 'subsample': 0.7207475893999605, 'colsample_bytree': 0.7085464185639322, 'reg_alpha': 0.6819447707779615, 'reg_lambda': 6.755501728949261}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:48:45,776] Trial 198 finished with value: 0.28490557756320484 and parameters: {'n_estimators': 1032, 'learning_rate': 0.01256799871746791, 'max_depth': 6, 'num_leaves': 25, 'min_child_samples': 53, 'subsample': 0.7766348564198745, 'colsample_bytree': 0.718944525240811, 'reg_alpha': 0.011907195521239864, 'reg_lambda': 4.497872471231627}. Best is trial 122 with value: 0.2853958763193618.\n",
      "[I 2025-06-27 18:49:32,234] Trial 199 finished with value: 0.2838695102372301 and parameters: {'n_estimators': 1070, 'learning_rate': 0.017258138411092423, 'max_depth': 6, 'num_leaves': 35, 'min_child_samples': 58, 'subsample': 0.7008940441822079, 'colsample_bytree': 0.7052071077039666, 'reg_alpha': 1.1205610923041252, 'reg_lambda': 7.773872355362545}. Best is trial 122 with value: 0.2853958763193618.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 1038, 'learning_rate': 0.013281377932256606, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 50, 'subsample': 0.7290404930495044, 'colsample_bytree': 0.7101352369500463, 'reg_alpha': 0.18001601412071802, 'reg_lambda': 4.7290215803443365}\n",
      "Best score: 0.2853958763193618\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'None',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 8),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 50, 200),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    train_data = train_df.drop(columns='id').copy()\n",
    "    train_data['reg03/reg02'] = train_data['ps_reg_03'] / (train_data['ps_reg_02'] + 1e-8)\n",
    "    train_data['car02cat/car07cat'] = train_data['ps_car_02_cat'] / (train_data['ps_car_07_cat'] + 1e-8)\n",
    "    train_data = train_data.drop(columns=rest_calc)\n",
    "    \n",
    "    X = train_data.drop(columns='target')\n",
    "    y = train_data['target']\n",
    "    \n",
    "    # 5-fold CV\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        pred = model.predict_proba(X_val)[:, 1]\n",
    "        score = gini_normalized(y_val, pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "80014b19-9e84-4a2b-9b5a-b7343a4a3c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200/200 완료\n",
      "현재 최고 점수: 0.28540\n",
      "현재 최고 파라미터: {'n_estimators': 1038, 'learning_rate': 0.013281377932256606, 'max_depth': 6, 'num_leaves': 28, 'min_child_samples': 50, 'subsample': 0.7290404930495044, 'colsample_bytree': 0.7101352369500463, 'reg_alpha': 0.18001601412071802, 'reg_lambda': 4.7290215803443365}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88a4fb-53b8-4cba-b612-522e7bdc6596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DriverPred)",
   "language": "python",
   "name": "driverpred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
